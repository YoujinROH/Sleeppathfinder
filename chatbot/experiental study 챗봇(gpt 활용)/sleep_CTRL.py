import gradio as gr
from openai import OpenAI
import os
import time
import json
import random
import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification
from dotenv import load_dotenv
from datetime import datetime
import re
import pandas as pd
import faiss
from sentence_transformers import SentenceTransformer

# ==================== ì—°êµ¬ì ì„¤ì • ====================
# ì—¬ê¸°ì„œ ì‹¤í—˜ ì¡°ê±´ì„ 'EXP' ë˜ëŠ” 'CTRL'ë¡œ ì„¤ì •í•˜ì„¸ìš”.
ASSIGNED_ARM = "CTRL"

# ==================== 0. OpenAI API ë° ëª¨ë¸ ì„¤ì • ====================
try:
    with open("OPENAI_API_KEY.txt", "r") as f:
        api_key = f.read().strip()
except FileNotFoundError:
    print("ì˜¤ë¥˜: OPENAI_API_KEY.txt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

if not api_key:
    print("ì˜¤ë¥˜: OPENAI_API_KEY.txt íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    exit()

load_dotenv()
HF_ACCESS_TOKEN = os.getenv('HF_ACCESS_TOKEN')
if HF_ACCESS_TOKEN is None:
    print("ì˜¤ë¥˜: HF_ACCESS_TOKEN í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("Hugging Face í† í°ì„ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •í•´ì•¼ ë¹„ê³µê°œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    exit()

client = OpenAI(api_key=api_key)

try:
    device = "cuda" if torch.cuda.is_available() else "cpu"
    ROBERTA_MODEL_NAME = "youjin129/roberta-cbti-finetuned"
    tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_MODEL_NAME, token=HF_ACCESS_TOKEN)
    model = RobertaForSequenceClassification.from_pretrained(ROBERTA_MODEL_NAME, token=HF_ACCESS_TOKEN).to(device)
    model.eval()
    LABELS = ["Sleep Hygiene", "Stimulus Control", "Sleep Restriction", "Relaxation Techniques", "Cognitive Restructuring"]
    print("âœ… í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ RoBERTa ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"ğŸš¨ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    print("ğŸ‘‰ í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ëª…ì´ ì˜¬ë°”ë¥¸ì§€, ê·¸ë¦¬ê³  HF_ACCESS_TOKEN í™˜ê²½ ë³€ìˆ˜ê°€ ìœ íš¨í•œì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    exit()

# ==================== RAG ëª¨ë¸ ë° ë°ì´í„° ì„¤ì • ====================
try:
    RAG_DATA_FILE = "./data/RAG_0407_eng.xlsx"
    df_rag = pd.read_excel(RAG_DATA_FILE)
    df_rag.columns = ['user_input', 'approach', 'info', 'rejection_message', 'intent']
    df_rag = df_rag.dropna(subset=['user_input'])
    embedding_model = SentenceTransformer('distiluse-base-multilingual-cased-v2')
    print("âœ… RAG ë°ì´í„° ë° ì„ë² ë”© ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
except FileNotFoundError:
    print(f"ì˜¤ë¥˜: RAG ë°ì´í„° íŒŒì¼ {RAG_DATA_FILE}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()
except Exception as e:
    print(f"ğŸš¨ RAG ë°ì´í„°/ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    exit()

# ==================== ëŒ€í™” ë‹¨ê³„ ì •ì˜ ====================
STAGE_NAME_INPUT = "name_input"
STAGE_PSYCHOEDUCATION_START = "psychoeducation_start"
STAGE_PSYCHOEDUCATION = "psychoeducation"
STAGE_PROBLEM_CONFIRM = "problem_confirm"
STAGE_USER_CONFIRMATION = "user_confirmation"
STAGE_WAIT_FOR_SOCRATIC_START = "wait_for_socratic_start"
STAGE_SOCRATIC_QUESTIONING = "socratic_questioning"
STAGE_MICRO_PE_AND_RQ1 = "micro_pe_and_rq1"
STAGE_RQ2_PLANNING = "rq2_planning"
STAGE_FINAL_PLAN_CONFIRM = "final_plan_confirm"

# ==================== CBT-I ê¸°ë²• ë°ì´í„° ====================
CBT_I_DESCRIPTIONS = {
    "Sleep Hygiene": "ìˆ˜ë©´ ìœ„ìƒ êµìœ¡ì€ ê±´ê°•í•œ ìˆ˜ë©´ì„ ìœ„í•´ ìƒí™œ ìŠµê´€ì„ ê°œì„ í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë‚®ì—ëŠ” ì¹´í˜ì¸ ì„­ì·¨ë¥¼ ì¤„ì´ê³ , ì¼ì •í•œ ì‹œê°„ì— ìê³  ì¼ì–´ë‚˜ë©°, ì·¨ì¹¨ ì „ì—ëŠ” ì „ìê¸°ê¸° ì‚¬ìš©ì„ í”¼í•˜ëŠ” ë“±ì˜ ìŠµê´€ì„ í¬í•¨í•©ë‹ˆë‹¤.",
    "Stimulus Control": "ìê·¹ ì¡°ì ˆ ìš”ë²•ì€ ì¹¨ëŒ€ë¥¼ ì˜¤ì§ ìˆ˜ë©´ë§Œì„ ìœ„í•œ ì¥ì†Œë¡œ ì¸ì‹í•˜ê²Œ ë§Œë“œëŠ” ì¹˜ë£Œë²•ì…ë‹ˆë‹¤. ì ì´ ì˜¤ì§€ ì•Šì„ ë•ŒëŠ” ì¦‰ì‹œ ì¹¨ëŒ€ì—ì„œ ë²—ì–´ë‚˜ê³ , ì¡¸ë¦´ ë•Œë§Œ ì¹¨ëŒ€ë¡œ ëŒì•„ê°€ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤.",
    "Sleep Restriction": "ìˆ˜ë©´ ì œí•œ ìš”ë²•ì€ ì¹¨ëŒ€ì— ë¨¸ë¬´ëŠ” ì‹œê°„ì„ ì˜ë„ì ìœ¼ë¡œ ì¤„ì—¬, ì¹¨ëŒ€ì™€ ìˆ˜ë©´ ì‚¬ì´ì˜ ì˜¬ë°”ë¥¸ ì—°ê²°ê³ ë¦¬ë¥¼ ì¬êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì‹¤ì œ ìˆ˜ë©´ ì‹œê°„ì„ ê³„ì‚°í•˜ì—¬ ì ì°¨ ì‹œê°„ì„ ëŠ˜ë ¤ê°‘ë‹ˆë‹¤.",
    "Relaxation Techniques": "ì´ì™„ ìš”ë²•ì€ ì‹¬ë¦¬ì Â·ì‹ ì²´ì  ê¸´ì¥ì„ ì™„í™”ì‹œì¼œ ìì—°ìŠ¤ëŸ¬ìš´ ìˆ˜ë©´ì„ ìœ ë„í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì‹¬í˜¸í¡, ëª…ìƒ, ê°€ë²¼ìš´ ìŠ¤íŠ¸ë ˆì¹­ ë“±ì„ í†µí•´ ëª¸ê³¼ ë§ˆìŒì„ í¸ì•ˆí•˜ê²Œ ë§Œë“œëŠ” ê²ƒì´ ì£¼ëœ ëª©í‘œì…ë‹ˆë‹¤.",
    "Cognitive Restructuring": "ì¸ì§€ì  ì¬êµ¬ì„±ì€ ìˆ˜ë©´ê³¼ ê´€ë ¨ëœ ë¶€ì •ì ì¸ ìƒê°ì´ë‚˜ ê±±ì •ì„ ê¸ì •ì ìœ¼ë¡œ ì „í™˜í•˜ëŠ” ì¹˜ë£Œë²•ì…ë‹ˆë‹¤. 'ì˜¤ëŠ˜ë„ ì ì„ ëª» ìë©´ í°ì¼ ë‚  ê±°ì•¼'ì™€ ê°™ì€ ìƒê° ëŒ€ì‹  ê¸ì •ì ì¸ ê´€ì ìœ¼ë¡œ ë³€í™”ì‹œí‚¤ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤."
}
KOR_LABELS = {
    "ìˆ˜ë©´ ìœ„ìƒ": "Sleep Hygiene",
    "ìê·¹ ì¡°ì ˆ": "Stimulus Control",
    "ìˆ˜ë©´ ì œí•œ": "Sleep Restriction",
    "ì´ì™„": "Relaxation Techniques",
    "ì¸ì§€ ì¬êµ¬ì„±": "Cognitive Restructuring"
}

# ==================== GPT í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (EXP/CTRL ë¶„ë¦¬) ====================
PROMPTS_EXP = {
    "rq1_explore_intro": """
ë‹¹ì‹ ì€ ê³µê°ì  ìˆ˜ë©´ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì‚¬ìš©ì ë°œì–¸: '{{user_input}}'ê³¼ ê´€ë ¨ëœ ë§¥ë½: '{{context}}'ì„ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìê°€ ìì‹ ì˜ ìˆ˜ë©´ ë¬¸ì œì— ëŒ€í•´ ìŠ¤ìŠ¤ë¡œ ë” ê¹Šì´ íƒìƒ‰í•˜ê³  ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” íƒìƒ‰í˜• ì§ˆë¬¸ì„ 1~2ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”. ê³µê°í•˜ëŠ” ë¬¸ì¥ê³¼ í•¨ê»˜, ìˆ˜ë©´ì˜ ì¤‘ìš”ì„±ì— ëŒ€í•œ í•œë‘ ë¬¸ì¥ ì •ë„ì˜ ì‹¬ë¦¬êµìœ¡ì„ í¬í•¨í•˜ì„¸ìš”.
""",
    "socratic_question_generator": """
ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ ìˆ˜ë©´ ì¸ì§€ í–‰ë™ ì¹˜ë£Œ(CBT-I) ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ ìì‹ ì˜ ìˆ˜ë©´ ë¬¸ì œì— ëŒ€í•´ ë” ê¹Šì´ ê³ ë¯¼í•˜ê³  ìŠ¤ìŠ¤ë¡œ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ì†Œí¬ë¼í…ŒìŠ¤ì‹ ì§ˆë¬¸ì„ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì´ì „ ë°œì–¸: '{{user_input}}'
ëŒ€í™” ë§¥ë½: '{{context}}'
ì§€ì •ëœ ì§ˆë¬¸ ìœ í˜•: '{{question_type}}'
ì‚¬ìš©ìì˜ ë°œì–¸ì— ëŒ€í•œ ê³µê°ê³¼ í•¨ê»˜, ì§€ì •ëœ ì§ˆë¬¸ ìœ í˜•ì— ë§ëŠ” ì§ˆë¬¸ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
""",
    "micro_pe_after_socratic": """
ì‚¬ìš©ìì˜ ë°œì–¸ê³¼ ëŒ€í™” ë§¥ë½ì„ ê¸°ë°˜ìœ¼ë¡œ, ì‚¬ìš©ìì˜ ë¬¸ì œê°€ '{{label}}' ê¸°ë²•ê³¼ ê´€ë ¨ì´ ìˆìŒì„ ì„¤ëª…í•˜ì„¸ìš”.
ì§ì ‘ì ì¸ ì¡°ì–¸ë³´ë‹¤ëŠ”, '{{label}}'ì´ ì‚¬ìš©ìê°€ ê²ªëŠ” ì–´ë ¤ì›€ì— ì–´ë–»ê²Œ ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ”ì§€ ê³µê°í•˜ëŠ” í†¤ìœ¼ë¡œ ì—°ê²°í•´ì£¼ì„¸ìš”.
ì‚¬ìš©ìê°€ ìì‹ ì˜ ìƒí™©ì„ ìƒˆë¡œìš´ ê´€ì ì—ì„œ ì¸ì‹í•˜ê³ , ë‹¤ìŒ ë‹¨ê³„ì˜ í–‰ë™ ê³„íšì— ëŒ€í•´ ìŠ¤ìŠ¤ë¡œ ìƒê°í•´ ë³¼ ìˆ˜ ìˆë„ë¡ ìœ ë„í•˜ì„¸ìš”.
""",
    "rq2_self_decision_rejection_handler": """
ì‚¬ìš©ìê°€ ì œì‹œëœ í–‰ë™ ê³„íš ì˜µì…˜ì— ëŒ€í•´ '{{rejection_reason}}'ê³¼ ê°™ì€ ì´ìœ ë¡œ ì–´ë ¤ì›€ì´ë‚˜ ë¶ˆí™•ì‹¤ì„±ì„ í‘œí˜„í–ˆìŠµë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ê°ì •ì„ ê³µê°í•˜ëŠ” ë¬¸ì¥ì„ ë§Œë“¤ê³ ,
'ì–´ë–¤ ì¢…ë¥˜ì˜ ê³„íšì´ ë” ë„ì›€ì´ ë  ê²ƒ ê°™ë‚˜ìš”? ì•„ë‹ˆë©´ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ê³„íšì„ ì„¸ì›Œë³¼ê¹Œìš”?'ì™€ ê°™ì´ ì‚¬ìš©ìì—ê²Œ ë” ì í•©í•œ ê³„íšì„ í•¨ê»˜ íƒìƒ‰í•˜ë„ë¡ ìœ ë„í•˜ëŠ” ì§ˆë¬¸ì„ 1~2ë¬¸ì¥ìœ¼ë¡œ ì œì‹œí•˜ì„¸ìš”ã€‚
ëŒ€í™” ë§¥ë½: '{{context}}'
""",
    "rq2_self_decision_followup_question": """
ì‚¬ìš©ìëŠ” í–‰ë™ ê³„íš ì˜µì…˜ì— ëŒ€í•´ '{{user_input}}'ê³¼ ê°™ì´ ì¶”ê°€ì ì¸ ì§ˆë¬¸ì´ë‚˜ ë°˜ì‘ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹µë³€ í›„, 'í˜¹ì‹œ ë‹¤ë¥¸ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹ ê°€ìš”?'ì™€ ê°™ì´ ëŒ€í™”ë¥¼ ì´ì–´ê°ˆ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ìœ¼ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.
ëŒ€í™” ë§¥ë½: '{{context}}'
""",
    "rq2_escalated_rejection_message": """
ì œì•ˆë“œë¦° ê³„íšë“¤ì´ í˜„ì¬ ë‹¹ì‹ ì—ê²ŒëŠ” ë¶€ë‹´ìŠ¤ëŸ½ê±°ë‚˜ ì ì ˆí•˜ì§€ ì•Šë‹¤ê³  ëŠë¼ì‹œëŠ”êµ°ìš”. ê·¸ëŸ° ë§ˆìŒì´ ë“œëŠ” ê²ƒì€ ì¶©ë¶„íˆ ì´í•´ê°€ ë©ë‹ˆë‹¤. ì§€ê¸ˆ ë‹¹ì¥ ì–´ë–¤ í–‰ë™ì„ ì •í•˜ëŠ” ê²ƒì´ ì–´ë µë‹¤ë©´, ì ì‹œ ì‰¬ì–´ê°€ê±°ë‚˜ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ê³ ë¯¼ì„ ì´ì–´ê°€ëŠ” ê²ƒë„ ê´œì°®ì•„ìš”. ëŒ€í™”ë¥¼ ë§ˆë¬´ë¦¬í• ê¹Œìš”? (ì˜ˆ/ì•„ë‹ˆì˜¤)
""",
    "rq2_rag_enhanced_prompt": """
ë‹¹ì‹ ì€ ìˆ˜ë©´ í–‰ë™ ê³„íš ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ë¬¸ì œëŠ” ì£¼ë¡œ **{{predicted_label}}** ê¸°ë²•ê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤.
ì‚¬ìš©ìì˜ í•µì‹¬ ë¬¸ì œì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {{problem_summary}}
ë‹¤ìŒì€ RAG ì‹œìŠ¤í…œì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:
{{retrieved_info}}
ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ê°€ì¥ ë„ì›€ì´ ë  ë§Œí•œ êµ¬ì²´ì ì¸ í–‰ë™ ê³„íš ì˜µì…˜ 3ê°€ì§€ë¥¼ 'â‘ ', 'â‘¡', 'â‘¢'ê³¼ ê°™ì´ ëª…í™•í•œ ë²ˆí˜¸ë¡œ ì œì‹œí•˜ì„¸ìš”. ê³„íšì€ ì‚¬ìš©ìì˜ í•µì‹¬ ë¬¸ì œì™€ ì—°ê´€ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
ì˜µì…˜ ì œì‹œ í›„, 'ê·¸ë¦¬ê³  ì´ ì¤‘ì—ì„œ ì–´ë–¤ ê²ƒì„ ì„ íƒí•˜ì‹œê² ì–´ìš”? ì™œ ê·¸ê±¸ ê³ ë¥´ì…¨ëŠ”ì§€ í•œ ì¤„ë¡œ ì ì–´ì£¼ì„¸ìš”.'ë¼ê³  ì§ˆë¬¸í•˜ì—¬ ì‚¬ìš©ìê°€ ì§ì ‘ ì„ íƒí•˜ê³  ê·¸ ì´ìœ ë¥¼ ë§í•˜ê²Œ ìœ ë„í•˜ì„¸ìš”.
""",
    "rq2_alternative_offer": """
ë„¤, ë¬¼ë¡ ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ë²•ë„ í•¨ê»˜ ê³ ë¯¼í•´ ë³¼ ìˆ˜ ìˆì–´ìš”. {{predicted_label}} ì™¸ì— ë˜ ë‹¤ë¥¸ ìˆ˜ë©´ ê¸°ë²•ì— ëŒ€í•´ ë” ì•Œì•„ë³¼ê¹Œìš”? ì•„ë‹ˆë©´ ì–´ë–¤ ì ì´ ë§ˆìŒì— ë“¤ì§€ ì•Šìœ¼ì…¨ëŠ”ì§€ ë” ìì„¸íˆ ì´ì•¼ê¸°í•´ ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?
"""
}

PROMPTS_CTRL = {
    "rq1_inform_intro": """
ë‹¹ì‹ ì€ ìˆ˜ë©´ ê±´ê°•ì— ëŒ€í•œ ì •ë³´ë¥¼ ê°„ê²°í•˜ê²Œ ì œê³µí•˜ëŠ” ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì‚¬ìš©ì ë°œì–¸: '{{user_input}}'ê³¼ ê´€ë ¨ëœ ë§¥ë½: '{{context}}'ì„ ë°”íƒ•ìœ¼ë¡œ, ë¶ˆë©´ì¦ì˜ ì •ì˜, ì¼ë°˜ì ì¸ ì§€í‘œ ë˜ëŠ” ìˆ˜ë©´ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ëŒ€í•´ 2~3ë¬¸ì¥ìœ¼ë¡œ ê°„ëµí•˜ê²Œ ì•ˆë‚´í•˜ì„¸ìš”.
""",
    "analysis_and_pe_and_rq1": """
ë‹¹ì‹ ì€ ìˆ˜ë©´ ìƒë‹´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ì ë¬¸ì œì˜ ì›ì¸ìœ¼ë¡œ '{{label}}'ì´ í™•ì •ë˜ì—ˆìŠµë‹ˆë‹¤.
ì´ '{{label}}' ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ 2~3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•œ ì‹¬ë¦¬êµìœ¡(Micro-PE)ì„ ì œê³µí•˜ì„¸ìš”.
""",
    "rq1_inform_question": """
í˜¹ì‹œ ìµœê·¼ì— ìŠ¤íŠ¸ë ˆìŠ¤ë‚˜ ìƒí™œ íŒ¨í„´ì˜ ë³€í™”ê°€ ìˆì—ˆë‚˜ìš”? ì•„ë‹ˆë©´ ì¼ìƒì—ì„œ ì–´ë–¤ ìˆ˜ë©´ í™˜ê²½ì„ ì¡°ì„±í•˜ê³  ê³„ì‹ ê°€ìš”?
""",
    "rq2_directive_command": """
ë‹¹ì‹ ì€ ìˆ˜ë©´ í–‰ë™ ê³„íš ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ë¬¸ì œëŠ” **{{predicted_label}}** ê¸°ë²•ê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. ì´ ë¬¸ì œì— ê°€ì¥ ì í•©í•œ **êµ¬ì²´ì ì¸ í–‰ë™ ê³„íšì„ ì˜¤ì§ í•œ ê°€ì§€**ë§Œ ëª…í™•í•˜ê²Œ ì§€ì‹œí•˜ëŠ” ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

ì˜ˆì‹œ ë‹µë³€:
- ì˜¤ëŠ˜ ë°¤ë¶€í„° ì ë“¤ê¸° ì „ 30ë¶„ ë™ì•ˆ ìŠ¤ë§ˆíŠ¸í° ì‚¬ìš©ì„ ë©ˆì¶”ê³  ì±…ì„ ì½ì–´ë³´ì„¸ìš”.
- ë§¤ì¼ ì•„ì¹¨ ê°™ì€ ì‹œê°„ì— ì¼ì–´ë‚˜ í–‡ë¹›ì„ ì¬ì–´ë³´ì„¸ìš”.

ì œì•ˆ í›„, 'ì´ í–‰ë™ì„ ì˜¤ëŠ˜ ì‹¤ì²œí•´ ë³´ì‹œê² ì–´ìš”?'ë¼ê³  ì§ˆë¬¸í•˜ì—¬ ì‚¬ìš©ìì˜ í™•ì¸ì„ ìš”ì²­í•˜ì„¸ìš”.
""",
    "rq2_rejection_and_alternative_offer": """
ì‚¬ìš©ìê°€ ì œì•ˆëœ í–‰ë™ ê³„íšì— ëŒ€í•´ ê±°ë¶€ ì˜ì‚¬ë¥¼ í‘œí˜„í–ˆìŠµë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ê±°ë¶€ ì´ìœ ë‚˜ ê°ì •ì— ê³µê°í•˜ëŠ” ì§§ì€ ë¬¸ì¥ì„ ë§Œë“¤ê³ , í˜„ì¬ ì œì•ˆí•œ {{predicted_label}} ê¸°ë²• ì™¸ì— ë‹¤ë¥¸ ìˆ˜ë©´ ê¸°ë²•(ì˜ˆ: ìê·¹ ì¡°ì ˆ, ìˆ˜ë©´ ì œí•œ)ë„ ìˆë‹¤ëŠ” ì ì„ ê°„ë‹¨íˆ ì–¸ê¸‰í•˜ë©°, ì„¸ì…˜ì„ ë¶€ë“œëŸ½ê²Œ ë§ˆë¬´ë¦¬í•˜ëŠ” ë©”ì‹œì§€ë¥¼ 1-2ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
""",
    "rq2_directive_escalated_rejection_message": """
ê·¸ë ‡êµ°ìš”. ì˜¤ëŠ˜ì€ ê³„íšì„ ì •í•˜ê¸° ì–´ë ¤ìš°ì‹  ê²ƒ ê°™ë„¤ìš”. ê´œì°®ìŠµë‹ˆë‹¤. ë‹¤ìŒì— ë‹¤ì‹œ ì‹œë„í•´ ë³¼ê¹Œìš”? ì„¸ì…˜ì„ ì¢…ë£Œí•˜ê² ìŠµë‹ˆë‹¤.
""",
    "rq2_directive_final_message_accept": """
ì¢‹ìŠµë‹ˆë‹¤. ë‹¹ì‹ ì˜ ìˆ˜ë©´ ê°œì„ ì„ ì‘ì›í•©ë‹ˆë‹¤. ëŒ€í™”ë¥¼ ë§ˆë¬´ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.
""",
    "rq2_directive_final_message_reject": """
ë„¤, ì•Œê² ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ë²•ì„ ì‹œë„í•˜ê³  ì‹¶ìœ¼ì‹  ê²ƒ ê°™ë„¤ìš”. ëŒ€í™”ë¥¼ ë§ˆë¬´ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.
""",
    "rq2_directive_final_message_no_intent": """
ê·¸ë ‡êµ°ìš”. ì˜¤ëŠ˜ì€ ê³„íšì„ ì •í•˜ê¸° ì–´ë ¤ìš°ì‹  ê²ƒ ê°™ë„¤ìš”. ëŒ€í™”ë¥¼ ë§ˆë¬´ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.
"""
}

PROMPTS_COMMON = {
    "socratic_type_selector": """
ë‹¹ì‹ ì€ ì†Œí¬ë¼í…ŒìŠ¤ì‹ ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ 5ê°€ì§€ ìœ í˜• ì¤‘ ì‚¬ìš©ì ë°œì–¸ì— ê°€ì¥ ì í•©í•œ ìœ í˜•ì„ ì„ íƒí•˜ì„¸ìš”:
- clarity: ë°œì–¸ì„ ëª…í™•íˆ í•˜ë„ë¡ ìœ ë„í•˜ëŠ” ì§ˆë¬¸ (ì˜ˆ: "ê·¸ê²Œ ì •í™•íˆ ë¬´ìŠ¨ ì˜ë¯¸ì¸ê°€ìš”?")
- assumptions: ìˆ¨ê²¨ì§„ ê°€ì •ì´ë‚˜ ë¯¿ìŒì„ íƒìƒ‰í•˜ëŠ” ì§ˆë¬¸ (ì˜ˆ: "ì–´ë–¤ ì „ì œë¥¼ í•˜ê³  ê³„ì‹ ê°€ìš”?")
- reasons_evidence: ì£¼ì¥ì˜ ê·¼ê±°ë¥¼ íƒìƒ‰í•˜ëŠ” ì§ˆë¬¸ (ì˜ˆ: "ì™œ ê·¸ë ‡ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?")
- implication_consequences: ë°œì–¸ì˜ ê²°ê³¼ë‚˜ í•¨ì˜ë¥¼ íƒìƒ‰í•˜ëŠ” ì§ˆë¬¸ (ì˜ˆ: "ì´ ìƒí™©ì´ ì§€ì†ë˜ë©´ ì–´ë–¤ ì¼ì´ ì¼ì–´ë‚  ê²ƒì´ë¼ê³  ì˜ˆìƒí•˜ì‹œë‚˜ìš”?")
- alternate_viewpoints_perspectives: ë‹¤ë¥¸ ê´€ì ì„ íƒìƒ‰í•˜ëŠ” ì§ˆë¬¸ (ì˜ˆ: "ì´ ë¬¸ì œë¥¼ ë‹¤ë¥¸ ê´€ì ì—ì„œ ë³¼ ìˆ˜ë„ ìˆì„ê¹Œìš”?")

ì‚¬ìš©ì ë°œì–¸: '{{user_input}}'
ëŒ€í™” ë§¥ë½: '{{context}}'

ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ ì˜¤ì§ ê°€ì¥ ì ì ˆí•œ **ìœ í˜•ì˜ ì˜ì–´ ì´ë¦„ë§Œ** ì¶œë ¥í•˜ì„¸ìš”. (ì˜ˆ: clarity, assumptions)
""",
    "confidence_check": """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ë°œì–¸ì„ ë¶„ì„í•˜ëŠ” AIì…ë‹ˆë‹¤. ë‹¤ìŒ ëŒ€í™” ë§¥ë½ê³¼ ì‚¬ìš©ì ë°œì–¸ì„ ê³ ë ¤í–ˆì„ ë•Œ, ì‚¬ìš©ìì˜ ìˆ˜ë©´ ë¬¸ì œ ì›ì¸ì´ ëª…í™•í•˜ê²Œ íŒŒì•…ë˜ì—ˆë‹¤ê³  ì–¼ë§ˆë‚˜ í™•ì‹ í•˜ë‚˜ìš”?
ì‚¬ìš©ì ë°œì–¸: '{{user_input}}'
ëŒ€í™” ë§¥ë½: '{{context}}'
ë‹¤ìŒ ì„¸ ê°€ì§€ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:
- 'high' (ì›ì¸ì´ ëª…í™•í•˜ê²Œ íŒŒì•…ë¨)
- 'middle' (ì›ì¸ì´ ì–´ëŠ ì •ë„ íŒŒì•…ë˜ì—ˆìœ¼ë‚˜ ë” ê¹Šì€ ëŒ€í™”ê°€ í•„ìš”í•¨)
- 'low' (ì›ì¸ íŒŒì•…ì´ ëª¨í˜¸í•¨)
""",
    "rq2_problem_summary": """
ë‹¹ì‹ ì€ ìˆ˜ë©´ ìƒë‹´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ë¬¸ì œì ì„ ê³µê°í•˜ë©° ê°„ê²°í•˜ê²Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.
ëŒ€í™” ë§¥ë½: '{{context}}'
""",
    "final_plan_confirm": """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ê³„íšì„ ì§€ì§€í•˜ê³  ê²©ë ¤í•˜ëŠ” ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì„ íƒí•œ ê³„íšê³¼ ê·¸ ì´ìœ ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë§¤ìš° ì§§ê³  ê¸ì •ì ì¸ ê²©ë ¤ ë©”ì‹œì§€ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
ì˜ˆì‹œ: "ë©‹ì§„ ê³„íšì…ë‹ˆë‹¤! ê¾¸ì¤€í•œ ì‹¤ì²œì„ ì‘ì›í•˜ê² ìŠµë‹ˆë‹¤."
""",
    "translate_ko_to_en": """
ì£¼ì–´ì§„ í•œêµ­ì–´ ë¬¸ì¥ì„ ì˜ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”. ë²ˆì—­ ì™¸ì—ëŠ” ë‹¤ë¥¸ ë§ì„ í•˜ì§€ ë§ˆì„¸ìš”.
""",
    "rq2_self_decision_intent_classifier": """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ í–‰ë™ ê³„íš ì„ íƒì— ëŒ€í•œ ì˜ë„ë¥¼ ë¶„ë¥˜í•˜ëŠ” AIì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì œì‹œëœ í–‰ë™ ê³„íš ì˜µì…˜ì— ëŒ€í•´ ì–´ë–¤ ë°˜ì‘ì„ ë³´ì˜€ëŠ”ì§€ ë‹¤ìŒ ë‘ ê°€ì§€ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:
- 'rejection_doubt' (ê±°ë¶€, ì˜ì‹¬, ë¶ˆí™•ì‹¤ì„±, ì–´ë ¤ì›€ í‘œí˜„)
- 'acceptance_selection' (ê³„íš ì„ íƒ, ë™ì˜, ì´ìœ  ì œì‹œ)

ì‚¬ìš©ì ë°œì–¸: '{{user_input}}'
ëŒ€í™” ë§¥ë½: '{{context}}'

ë‹¤ìŒ ì˜ˆì‹œë“¤ì„ ì°¸ê³ í•˜ì—¬ ë¶„ë¥˜í•˜ì„¸ìš”:
- "í•´ë³¼ê¹Œ" ë˜ëŠ” "ê³ ë¯¼í•´ë³¼ê²Œ"ì™€ ê°™ì´ ì‹¤í–‰ì— ëŒ€í•œ ê¸ì •ì  ì˜ì§€ë¥¼ ë‚´ë¹„ì¹˜ëŠ” ë°œì–¸ì€ 'acceptance_selection'ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
- "ì˜ ëª¨ë¥´ê² ì–´" ë˜ëŠ” "ê·¸ê±´ ì¢€ í˜ë“ ë°"ì™€ ê°™ì´ ë¶ˆí™•ì‹¤ì„±ì´ë‚˜ ì–´ë ¤ì›€ì„ í‘œí˜„í•˜ëŠ” ë°œì–¸ì€ 'rejection_doubt'ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
- "ì¼ê¸°ë¥¼ ì“¸ê¹Œë´"ì™€ ê°™ì´ "~í• ê¹Œë´" í˜•íƒœì˜ í‘œí˜„ì€ ê¸ì •ì  ì˜ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ 'acceptance_selection'ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.

ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ ì˜¤ì§ ë¶„ë¥˜ ê²°ê³¼ë§Œ ì˜ì–´ë¡œ ì¶œë ¥í•˜ì„¸ìš”. (ì˜ˆ: rejection_doubt)
""",
    "rq2_directive_rejection_handler": """
ì‚¬ìš©ìê°€ ì œì‹œëœ í–‰ë™ ê³„íšì„ ê±°ë¶€í–ˆìŠµë‹ˆë‹¤. ì‚¬ìš©ìì˜ ê±°ë¶€ ì´ìœ : '{{rejection_reason}}'ì„ ê³µê°í•˜ë©° ì´í•´í•˜ëŠ” ë¬¸ì¥ì„ ë§Œë“¤ê³ ,
'ê·¸ë ‡ë‹¤ë©´ ì–´ë–¤ ì ì´ ë¶€ë‹´ìŠ¤ëŸ¬ì› ë‚˜ìš”? ë‹¤ë¥¸ ë°©ë²•ì€ ì—†ì„ì§€ í•¨ê»˜ ê³ ë¯¼í•´ ë³¼ê¹Œìš”?'ì™€ ê°™ì´ ì‚¬ìš©ìì—ê²Œ ëŒ€ì•ˆì„ íƒìƒ‰í•˜ê±°ë‚˜ ì–´ë ¤ì›€ì„ ë” ê³µìœ í•˜ë„ë¡ ìœ ë„í•˜ëŠ” ì§ˆë¬¸ì„ 1~2ë¬¸ì¥ìœ¼ë¡œ ì œì‹œí•˜ì„¸ìš”.
ëŒ€í™” ë§¥ë½: '{{context}}'
""",
    "rq2_user_intent_classifier": """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ë°œì–¸ ì˜ë„ë¥¼ ë¶„ë¥˜í•˜ëŠ” AIì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ë°œì–¸ì´ ë‹¤ìŒ ì¤‘ ì–´ë–¤ ì˜ë„ì— ê°€ì¥ ê°€ê¹Œìš´ì§€ ë¶„ë¥˜í•˜ì„¸ìš”:
- 'request_alternatives' (ê¸°ì¡´ ì œì•ˆê³¼ ë‹¤ë¥¸ ëŒ€ì•ˆì„ ìš”ì²­í•¨. ì˜ˆ: "ë‹¤ë¥¸ ë°©ì‹ì€ ì—†ì„ê¹Œ?")
- 'direct_rejection' (ëª…í™•í•˜ê²Œ ì œì•ˆì„ ê±°ë¶€í•¨. ì˜ˆ: "ì‹«ì–´ìš”.", "ì•ˆ í• ë˜ìš”.")
- 'agreement_or_elaboration' (ë™ì˜í•˜ê±°ë‚˜, ì¶”ê°€ ì„¤ëª…ì„ í•¨. ì˜ˆ: "ë„¤.", "ê·¸ë ‡ê²Œ ìƒê°í•´ìš”.", "ë” ìì„¸íˆ ì•Œë ¤ì£¼ì„¸ìš”.")

ì‚¬ìš©ì ë°œì–¸: '{{user_input}}'
ëŒ€í™” ë§¥ë½: '{{context}}'
ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ ì˜¤ì§ ë¶„ë¥˜ ê²°ê³¼ë§Œ ì˜ì–´ë¡œ ì¶œë ¥í•˜ì„¸ìš”. (ì˜ˆ: request_alternatives)
"""
}

# ==================== ì „ì—­ ìƒìˆ˜ ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ====================
YES = {"ì˜ˆ", "ë„¤", "ì‘", "ë§ì•„ìš”", "ì¢‹ì•„ìš”", "ê·¸ë˜ìš”", "ë„µ"}
NO = {"ì•„ë‹ˆì˜¤", "ì•„ë‹ˆìš”", "ì•„ë‡¨", "ì‹«ì–´ìš”", "ì›ì¹˜ ì•Šì•„ìš”", "ë…¸"}

def yn_intent(text):
    t = text.strip().replace(" ", "")
    # 'ì‹«ì–´'ë¥¼ 'ì•„ë‹ˆì˜¤'ì™€ ë™ì˜ì–´ë¡œ ê°„ì£¼í•˜ì—¬ ì²˜ë¦¬
    if "ì‹«ì–´" in t or any(n in t for n in NO):
        return "N"
    if any(y in t for y in YES):
        return "Y"
    return None

def count_reason_sentences(text: str) -> int:
    sentences = re.split(r'[.!?]\s*', text)
    count = 0
    for s in sentences:
        s_stripped = s.strip()
        if len(s_stripped) > 8 and any(keyword in s_stripped for keyword in ["ì™œ", "ë•Œë¬¸", "ì´ìœ "]):
            count += 1
    return count

def count_goal_sentences(text: str) -> int:
    sentences = re.split(r'[.!?]\s*', text)
    count = 0
    for s in sentences:
        s_stripped = s.strip()
        if any(keyword in s_stripped for keyword in ["í•˜ê² ", "í•´ë³¼ê²Œ", "ì•Šê² ", "ì‹¤ì²œ", "ë…¸ë ¥"]):
            count += 1
    return count

def count_plan_sentences(text: str) -> int:
    sentences = re.split(r'[.!?]\s*', text)
    return sum(1 for s in sentences if len(s.strip()) > 8)

def assign_conditions(state):
    state["arm"] = ASSIGNED_ARM
    
    if state["arm"] == "EXP":
        state["rq1_mode"] = "explore"
        state["rq2_mode"] = "self_decision"
        state["rq3_mode"] = "exploratory"
        state["max_socratic_depth"] = 4
    else: # CTRL
        state["rq1_mode"] = "inform"
        state["rq2_mode"] = "directive"
        state["rq3_mode"] = "prescriptive"
        state["max_socratic_depth"] = 1

    log_interaction(state["log_file_path"],
        f"[ì‹¤í—˜ë°°ì •] ARM={state['arm']}, RQ1={state['rq1_mode']}, RQ2={state['rq2_mode']}, RQ3={state['rq3_mode']}")
    print(f"[ì‹¤í—˜ë°°ì •] ARM={state['arm']}, RQ1={state['rq1_mode']}, RQ2={state['rq2_mode']}, RQ3={state['rq3_mode']}")

def setup_logging(state):
    log_dir = "./logs_CTRL"
    os.makedirs(log_dir, exist_ok=True)
    user_name = state.get('user_name', 'anonymous')
    participant_id = state.get('participant_id', 'unknown_pid') 
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"{log_dir}/{user_name}_{participant_id}_{timestamp}.txt"
    state['log_file_path'] = log_filename
    state['start_time'] = time.time()
    with open(log_filename, "a", encoding="utf-8") as f:
        f.write(f"--- ëŒ€í™” ì‹œì‘: {timestamp} (ì‚¬ìš©ì: {user_name}, PID: {participant_id}) ---\n\n")
    print(f"âœ… ë¡œê·¸ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {log_filename}")

def log_interaction(log_file_path, message, tag=None):
    if log_file_path:
        timestamp = datetime.now().strftime("%H:%M:%S")
        tag_str = f"[{tag}] " if tag else ""
        with open(log_file_path, "a", encoding="utf-8") as f:
            f.write(f"[{timestamp}] {tag_str}{message}\n")

def call_gpt_api(system_prompt_content, user_message_content, model="gpt-4o-mini", conversation_history=None):
    messages = [{"role": "system", "content": system_prompt_content}]
    if conversation_history:
        messages.extend(conversation_history)
    messages.append({"role": "user", "content": user_message_content})
    try:
        completion = client.chat.completions.create(
            model=model,
            messages=messages
        )
        return completion.choices[0].message.content
    except Exception as e:
        print(f"ğŸš¨ GPT API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return f"GPT API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

def select_socratic_type(user_input, context, state):
    prompt_content = PROMPTS_COMMON["socratic_type_selector"].replace("{{user_input}}", user_input).replace("{{context}}", context)
    response = call_gpt_api(prompt_content, user_input, model="gpt-4o-mini")
    valid_types = ["clarity", "assumptions", "reasons_evidence", "implication_consequences", "alternate_viewpoints_perspectives"]
    selected_type = response.strip().lower()
    log_interaction(state.get('log_file_path'), f"[ì‹œìŠ¤í…œ] ì†Œí¬ë¼í…ŒìŠ¤ ìœ í˜• ì„ íƒ: {selected_type}", tag="socratic_type")
    if selected_type not in valid_types:
        return "clarity"
    return selected_type

def generate_socratic_question(user_input, context, question_type, state):
    prompt_content = PROMPTS_EXP["socratic_question_generator"].replace("{{user_input}}", user_input).replace("{{context}}", context).replace("{{question_type}}", question_type)
    response = call_gpt_api(prompt_content, user_input, model="gpt-4o-mini")
    log_interaction(state.get('log_file_path'), f"[ì‹œìŠ¤í…œ] ì†Œí¬ë¼í…ŒìŠ¤ ì§ˆë¬¸ ìƒì„± (ìœ í˜•: {question_type}): {response}", tag="socratic_gen")
    return response

def classify_with_bert(text, state):
    translation_prompt_content = PROMPTS_COMMON["translate_ko_to_en"]
    translated_text = call_gpt_api(translation_prompt_content, text, model="gpt-4o-mini")
    print(f"ğŸ‘‰ GPTê°€ ë²ˆì—­í•œ í…ìŠ¤íŠ¸: {translated_text}")
    log_interaction(state.get('log_file_path'), f"[ì‹œìŠ¤í…œ] BERT ì…ë ¥ ë²ˆì—­: {translated_text}", tag="bert_translate")
    inputs = tokenizer(translated_text, return_tensors="pt", truncation=True, padding=True).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = torch.nn.functional.softmax(logits, dim=-1)
        conf, predicted_idx = torch.max(probs, dim=1)
    label = LABELS[predicted_idx.item()]
    conf = conf.item()
    HIGH_CONF_TH = 0.55
    confidence_status = "ë†’ìŒ" if conf >= HIGH_CONF_TH else "ë‚®ìŒ"
    log_interaction(state.get('log_file_path'), f"[ì‹œìŠ¤í…œ] BERT ë¶„ë¥˜ ê²°ê³¼: ë¼ë²¨='{label}', ì‹ ë¢°ë„='{confidence_status}'", tag="bert_classify")
    return label, confidence_status, probs.squeeze()

def retrieve_rag_info(query, approach, state, top_k=1):
    if 'faiss_index_cache' not in state:
        state['faiss_index_cache'] = {}
    
    if approach not in state['faiss_index_cache']:
        filtered_df = df_rag[df_rag['approach'] == approach].copy()
        if filtered_df.empty:
            return ""

        embeddings = embedding_model.encode(filtered_df['info'].tolist(), convert_to_tensor=True).cpu().numpy()
        index = faiss.IndexFlatL2(embeddings.shape[1])
        index.add(embeddings)
        state['faiss_index_cache'][approach] = (index, filtered_df)
    
    index, filtered_df = state['faiss_index_cache'][approach]
    
    translated_query = call_gpt_api(PROMPTS_COMMON["translate_ko_to_en"], query, model="gpt-4o-mini")
    query_vector = embedding_model.encode([translated_query], convert_to_tensor=True).cpu().numpy()
    
    distances, indices = index.search(query_vector, top_k)
    
    if indices.size > 0:
        return filtered_df.iloc[indices[0][0]]['info']
    return ""


def export_session_summary(state, final_message):
    summary_dir = "./session_summaries"
    os.makedirs(summary_dir, exist_ok=True)
    
    end_time = time.time()
    elapsed_seconds = end_time - state.get('start_time', end_time)
    
    summary = {
        "arm": state.get('arm'),
        "survey_token": state.get('participant_id'),
        "predicted_label": state.get('predicted_label'),
        "rq1_mode": state.get('rq1_mode'),
        "rq2_mode": state.get('rq2_mode'),
        "rq3_mode": state.get('rq3_mode'),
        "socratic_turns": state.get('socratic_turns', 0),
        "inform_turns": state.get('inform_turns', 0),
        "directive_accepts": state.get('directive_accepts', 0),
        "directive_denies": state.get('directive_denies', 0),
        "rq2_rejection_count": state.get('rq2_rejection_count', 0),
        "total_turns": len(state.get('history', [])) / 2,
        "elapsed_seconds": round(elapsed_seconds, 2),
        "session_ended_at": datetime.now().isoformat(),
        "final_message": final_message
    }
    
    summary_filename = f"{summary_dir}/{state['participant_id']}.jsonl"
    with open(summary_filename, 'a', encoding='utf-8') as f:
        f.write(json.dumps(summary, ensure_ascii=False) + '\n')
    print(f"âœ… ì„¸ì…˜ ìš”ì•½ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {summary_filename}")


def _end_session(state, chat_history, user_input, final_message, psychoeducation_row):
    final_plan_response = call_gpt_api(
        PROMPTS_COMMON["final_plan_confirm"],
        final_message,
        model="gpt-4o-mini",
        conversation_history=chat_history
    )
    chat_history.append({"role": "assistant", "content": final_plan_response})
    log_interaction(state.get('log_file_path'), f"[ì±—ë´‡]: {final_plan_response}", tag="final_plan_confirm")
    
    final_label = state.get('predicted_label', 'ë¯¸ë¶„ë¥˜')
    log_interaction(state.get('log_file_path'), f"[ì‹œìŠ¤í…œ] ìµœì¢… ì ìš© CBT-I ê¸°ë²•: {final_label}", tag="final_cbt_i_technique")
    
    state['session_ended_at'] = datetime.now().isoformat()
    log_interaction(state.get('log_file_path'), f"[ì‹œìŠ¤í…œ] ì„¸ì…˜ ì¢…ë£Œ.", tag="session_end")

    export_session_summary(state, final_plan_response)
    
    state['stage'] = STAGE_FINAL_PLAN_CONFIRM
    return "", chat_history, state, gr.update(visible=False)

def classify_user_plan_intent(user_input, context, state):
    prompt_content = PROMPTS_COMMON["rq2_self_decision_intent_classifier"].replace("{{user_input}}", user_input).replace("{{context}}", context)
    response = call_gpt_api(prompt_content, user_input, model="gpt-4o-mini")
    classified_intent = response.strip().lower()
    log_interaction(state.get('log_file_path'), f"[ì‹œìŠ¤í…œ] ê³„íš ì˜ë„ ë¶„ë¥˜ ê²°ê³¼: {classified_intent}", tag="plan_intent_classify")
    return classified_intent

def classify_user_micro_pe_intent(user_input, context, state):
    prompt_content = PROMPTS_COMMON["rq2_user_intent_classifier"].replace("{{user_input}}", user_input).replace("{{context}}", context)
    response = call_gpt_api(prompt_content, user_input, model="gpt-4o-mini")
    classified_intent = response.strip().lower()
    log_interaction(state.get('log_file_path'), f"[ì‹œìŠ¤í…œ] Micro-PE ì˜ë„ ë¶„ë¥˜ ê²°ê³¼: {classified_intent}", tag="user_intent_classify_pe")
    return classified_intent

def user_input_handler(user_input, state):
    new_state = state.copy()
    chat_history = new_state.get('history', [])
    current_stage = new_state.get('stage', STAGE_NAME_INPUT)
    
    if current_stage not in [STAGE_NAME_INPUT, STAGE_PSYCHOEDUCATION_START]:
        chat_history.append({"role": "user", "content": user_input})
        log_interaction(new_state.get('log_file_path'), f"[ì‚¬ìš©ì]: {user_input}", tag="user_input")

    if current_stage == STAGE_PSYCHOEDUCATION_START:
        new_state['stage'] = STAGE_PSYCHOEDUCATION
        bot_message = "ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ ê¸°ë²• ì„¤ëª…ì„ ë³¼ ìˆ˜ ìˆì–´ìš”. ì›í•˜ì‹œë©´ 'ì¤€ë¹„'ë¥¼ ì…ë ¥í•´ì„œ ë°”ë¡œ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì…”ë„ ë©ë‹ˆë‹¤."
        chat_history.append({"role": "assistant", "content": bot_message})
        log_interaction(new_state.get('log_file_path'), f"[ì±—ë´‡]: {bot_message}", tag="bot_response")
        return "", chat_history, new_state, gr.update(visible=True)

    elif current_stage == STAGE_PSYCHOEDUCATION:
        if user_input == "ì¤€ë¹„":
            new_state['stage'] = STAGE_PROBLEM_CONFIRM
            bot_message = "ì¢‹ìŠµë‹ˆë‹¤. ì–´ì œë‚˜ ì˜¤ëŠ˜, ì ê³¼ ê´€ë ¨í•´ ê°€ì¥ ë¶ˆí¸í•˜ê±°ë‚˜ ë§ˆìŒì— ê±¸ë ¸ë˜ ì ì´ ìˆì—ˆì„ê¹Œìš”?"
            chat_history.append({"role": "assistant", "content": bot_message})
            log_interaction(new_state.get('log_file_path'), f"[ì±—ë´‡]: {bot_message}", tag="bot_problem_confirm")
            return "", chat_history, new_state, gr.update(visible=False)
        
        elif user_input in KOR_LABELS.keys():
            english_label = KOR_LABELS[user_input]
            bot_message = CBT_I_DESCRIPTIONS[english_label]
            chat_history.append({"role": "assistant", "content": bot_message})
            log_interaction(new_state.get('log_file_path'), f"[ì±—ë´‡]: {bot_message}", tag="pe_info")
            
            bot_message_2 = "ë‹¤ë¥¸ ê¸°ë²•ë„ ê¶ê¸ˆí•˜ì‹œë©´ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”. ì¤€ë¹„ë˜ì…¨ìœ¼ë©´ 'ì¤€ë¹„'ë¼ê³  ì…ë ¥í•˜ê±°ë‚˜ ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”."
            chat_history.append({"role": "assistant", "content": bot_message_2})
            log_interaction(new_state.get('log_file_path'), f"[ì±—ë´‡]: {bot_message_2}", tag="pe_ready_prompt")
            return "", chat_history, new_state, gr.update(visible=True)
            
        else:
            bot_message = "ë²„íŠ¼ì„ ëˆ„ë¥´ê±°ë‚˜ 'ì¤€ë¹„'ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”."
            chat_history.append({"role": "assistant", "content": bot_message})
            log_interaction(new_state.get('log_file_path'), f"[ì±—ë´‡]: {bot_message}", tag="bot_invalid_pe_input")
            return "", chat_history, new_state, gr.update(visible=True)

    elif current_stage == STAGE_PROBLEM_CONFIRM:
        new_state['initial_problem_statement'] = user_input
        
        if new_state["arm"] == "EXP":
            explore_prompt_content = PROMPTS_EXP["rq1_explore_intro"].replace("{{user_input}}", user_input).replace("{{context}}", "")
            bot_message = call_gpt_api(explore_prompt_content, user_input, model="gpt-4o-mini", conversation_history=chat_history)
            chat_history.append({"role":"assistant","content":bot_message})
            log_interaction(new_state.get('log_file_path'), f"[ì±—ë´‡]: {bot_message}", tag="rq1_explore_q_intro")
            new_state['stage'] = STAGE_WAIT_FOR_SOCRATIC_START
            new_state['socratic_turns'] = 1
            return "", chat_history, new_state, gr.update(visible=False)
        else: # CTRL
            label, _, _ = classify_with_bert(user_input, new_state)
            new_state['predicted_label'] = label
            
            # CBT-I ê¸°ë²• ì„¤ëª…ê³¼ ì§ˆë¬¸ì„ í•©ì¹œ í†µí•© í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            pe_and_question_prompt = f"""
ë‹¹ì‹ ì€ ìˆ˜ë©´ ìƒë‹´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ì ë°œì–¸: '{{user_input}}'ì„ ë°”íƒ•ìœ¼ë¡œ,
1. ì‚¬ìš©ìì˜ ë¬¸ì œ ì›ì¸ìœ¼ë¡œ '{{label}}'ì´ í™•ì •ë˜ì—ˆìŒì„ ì•Œë¦¬ê³  ì´ì— ëŒ€í•œ 2-3ë¬¸ì¥ì˜ ê°„ê²°í•œ ì‹¬ë¦¬êµìœ¡(Micro-PE)ì„ ì œê³µí•˜ì„¸ìš”.
2. ì´í›„, ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš©ìì—ê²Œ 'í˜¹ì‹œ ìµœê·¼ì— ìŠ¤íŠ¸ë ˆìŠ¤ë‚˜ ìƒí™œ íŒ¨í„´ì˜ ë³€í™”ê°€ ìˆì—ˆë‚˜ìš”? ì•„ë‹ˆë©´ ì¼ìƒì—ì„œ ì–´ë–¤ ìˆ˜ë©´ í™˜ê²½ì„ ì¡°ì„±í•˜ê³  ê³„ì‹ ê°€ìš”?'ì™€ ê°™ì€ í›„ì† ì§ˆë¬¸ì„ ì´ì–´ë¶™ì´ì„¸ìš”.
"""
            combined_prompt_content = pe_and_question_prompt.replace("{{user_input}}", user_input).replace("{{label}}", label)
            
            bot_message = call_gpt_api(combined_prompt_content, user_input, model="gpt-4o-mini", conversation_history=chat_history)
            chat_history.append({"role": "assistant", "content": bot_message})
            log_interaction(new_state.get('log_file_path'), f"[ì±—ë´‡]: {bot_message}", tag="bot_pe_and_question_combined")

            new_state['inform_turns'] = 1
            new_state['stage'] = STAGE_RQ2_PLANNING
            return "", chat_history, new_state, gr.update(visible=False)
    
    elif current_stage == STAGE_WAIT_FOR_SOCRATIC_START:
        new_state['socratic_session_initial_input'] = user_input
        new_state['socratic_hints'] = []
        context_for_socratic_init = new_state.get('initial_problem_statement', '') + " " + user_input
        selected_type = select_socratic_type(user_input, context_for_socratic_init, new_state)
        bot_message = generate_socratic_question(user_input, context_for_socratic_init, selected_type, new_state)
        chat_history.append({"role": "assistant", "content": bot_message})
        log_interaction(new_state.get('log_file_path'), f"[ì±—ë´‡]: {bot_message}", tag="socratic_q_first")
        new_state['stage'] = STAGE_SOCRATIC_QUESTIONING
        new_state['socratic_turns'] = new_state.get('socratic_turns', 0) + 1

        return "", chat_history, new_state, gr.update(visible=False)
    
    elif current_stage == STAGE_SOCRATIC_QUESTIONING:
        new_state['socratic_hints'].append(user_input)
        
        context_for_confidence_list = [new_state.get('initial_problem_statement', '')]
        if new_state.get('socratic_session_initial_input'):
            context_for_confidence_list.append(new_state['socratic_session_initial_input'])
        context_for_confidence_list.extend(new_state.get('socratic_hints', []))
        full_context_for_gpt = " ".join(context_for_confidence_list).strip()
        
        confidence_prompt_content = PROMPTS_COMMON['confidence_check'].replace("{{user_input}}", user_input).replace("{{context}}", full_context_for_gpt)
        confidence = call_gpt_api(confidence_prompt_content, user_input, model="gpt-4o-mini").strip().lower()
        log_interaction(new_state.get('log_file_path'), f"[ì‹œìŠ¤í…œ] ì‹ ë¢°ë„ í™•ì¸ ê²°ê³¼: {confidence}", tag="confidence_check")
        
        max_depth = new_state.get("max_socratic_depth")
        
        if confidence == 'high' or len(new_state['socratic_hints']) >= max_depth:
            combined_query_parts = [new_state.get('initial_problem_statement', '')]
            if new_state.get('socratic_session_initial_input'):
                combined_query_parts.append(new_state['socratic_session_initial_input'])
            combined_query_parts.extend(new_state.get('socratic_hints', []))
            combined_query = " ".join(combined_query_parts).strip()
            
            label, _, _ = classify_with_bert(combined_query, new_state)
            new_state['predicted_label'] = label
            
            pe_prompt_content = PROMPTS_EXP["micro_pe_after_socratic"].replace("{{label}}", label)
            bot_message = call_gpt_api(pe_prompt_content, user_input, model="gpt-4o-mini", conversation_history=chat_history)
            chat_history.append({"role": "assistant", "content": bot_message})
            log_interaction(new_state.get('log_file_path'), f"[ì±—ë´‡]: {bot_message}", tag="micro_pe_after_socratic")
            new_state['stage'] = STAGE_MICRO_PE_AND_RQ1
        else:
            selected_type = select_socratic_type(user_input, full_context_for_gpt, new_state)
            bot_message = generate_socratic_question(user_input, full_context_for_gpt, selected_type, new_state)
            chat_history.append({"role": "assistant", "content": bot_message})
            log_interaction(new_state.get('log_file_path'), f"[ì±—ë´‡]: {bot_message}", tag="socratic_q_followup")
            new_state['stage'] = STAGE_SOCRATIC_QUESTIONING
            new_state['socratic_turns'] = new_state.get('socratic_turns', 0) + 1

        return "", chat_history, new_state, gr.update(visible=False)
    
    elif current_stage == STAGE_MICRO_PE_AND_RQ1:
        problem_summary_text_list = [new_state.get('initial_problem_statement', '')]
        if new_state.get('socratic_session_initial_input'):
            problem_summary_text_list.append(new_state['socratic_session_initial_input'])
        problem_summary_text_list.extend(new_state.get('socratic_hints', []))
        problem_summary_text = " ".join(problem_summary_text_list).strip()
        
        summary_prompt = PROMPTS_COMMON["rq2_problem_summary"].replace("{{context}}", problem_summary_text)
        problem_summary = call_gpt_api(summary_prompt, "", model="gpt-4o-mini")
        
        if new_state["arm"] == "EXP":
            user_intent = classify_user_micro_pe_intent(user_input, problem_summary_text, new_state)
            
            if user_intent == 'request_alternatives':
                bot_message = PROMPTS_EXP["rq2_alternative_offer"].replace("{{predicted_label}}", new_state['predicted_label'])
                chat_history.append({"role": "assistant", "content": bot_message})
                log_interaction(new_state.get('log_file_path'), f"[ì±—ë´‡]: {bot_message}", tag="rq2_alternative_offer")
                return "", chat_history, new_state, gr.update(visible=False)
            
            elif user_intent == 'agreement_or_elaboration':
                retrieved_rag_info = retrieve_rag_info(problem_summary_text, new_state['predicted_label'], new_state)
                prompt_content = PROMPTS_EXP["rq2_rag_enhanced_prompt"].replace("{{predicted_label}}", new_state['predicted_label']).replace("{{problem_summary}}", problem_summary).replace("{{retrieved_info}}", retrieved_rag_info)
                bot_message = call_gpt_api(prompt_content, user_input, model="gpt-4o-mini", conversation_history=chat_history)
                chat_history.append({"role": "assistant", "content": bot_message})
                log_interaction(new_state.get('log_file_path'), f"[ì±—ë´‡]: {bot_message}", tag="rq2_prompt_self_decision_rag")
                new_state['stage'] = STAGE_RQ2_PLANNING
                return "", chat_history, new_state, gr.update(visible=False)

            else: # direct_rejection í˜¹ì€ ì˜ˆìƒì¹˜ ëª»í•œ ì˜ë„
                retrieved_rag_info = retrieve_rag_info(problem_summary_text, new_state['predicted_label'], new_state)
                prompt_content = PROMPTS_EXP["rq2_rag_enhanced_prompt"].replace("{{predicted_label}}", new_state['predicted_label']).replace("{{problem_summary}}", problem_summary).replace("{{retrieved_info}}", retrieved_rag_info)
                bot_message = call_gpt_api(prompt_content, user_input, model="gpt-4o-mini", conversation_history=chat_history)
                chat_history.append({"role": "assistant", "content": bot_message})
                log_interaction(new_state.get('log_file_path'), f"[ì±—ë´‡]: {bot_message}", tag="rq2_prompt_self_decision_rag")
                new_state['stage'] = STAGE_RQ2_PLANNING
                return "", chat_history, new_state, gr.update(visible=False)
        
        else: # CTRL
            pe_question_prompt = PROMPTS_CTRL["rq1_inform_question"]
            bot_message = call_gpt_api(pe_question_prompt, user_input, model="gpt-4o-mini", conversation_history=chat_history)
            chat_history.append({"role": "assistant", "content": bot_message})
            log_interaction(new_state.get('log_file_path'), f"[ì±—ë´‡]: {bot_message}", tag="rq1_inform_question")

            new_state['stage'] = STAGE_RQ2_PLANNING
            return "", chat_history, new_state, gr.update(visible=False)
        
    elif current_stage == STAGE_RQ2_PLANNING:
        new_state['reasons_count'] += count_reason_sentences(user_input)
        new_state['goals_count'] += count_goal_sentences(user_input)
        new_state['plan_sentences_count'] += count_plan_sentences(user_input)
        log_interaction(new_state.get('log_file_path'),
                        f"[ì‹œìŠ¤í…œ] ì¹´ìš´í„° ì—…ë°ì´íŠ¸: ì´ìœ ={new_state['reasons_count']}, ëª©í‘œ={new_state['goals_count']}, ê³„íšë¬¸ì¥={new_state['plan_sentences_count']}", tag="counter_update")
        
        problem_summary_parts = [new_state.get('initial_problem_statement', '')]
        if new_state.get('socratic_session_initial_input'):
            problem_summary_parts.append(new_state['socratic_session_initial_input'])
        if new_state.get('socratic_hints'):
            problem_summary_parts.extend(new_state['socratic_hints'])
        problem_summary = " ".join(problem_summary_parts).strip()
        
        if new_state["arm"] == "EXP":
            user_input_lower = user_input.lower()
            if any(word in user_input_lower for word in ['ê±±ì •', 'êº¼ë ¤', 'ë¶ˆì•ˆ', 'ë‘ë ¤ì›€']):
                new_state['rq2_rejection_count'] = 0
                new_state['initial_problem_statement'] = user_input
                new_state['socratic_hints'] = []
                selected_type = select_socratic_type(user_input, user_input, new_state)
                bot_message = generate_socratic_question(user_input, user_input, selected_type, new_state)
                chat_history.append({"role": "assistant", "content": bot_message})
                log_interaction(new_state.get('log_file_path'), f"[ì±—ë´‡]: {bot_message}", tag="socratic_q_re-engage")
                new_state['stage'] = STAGE_SOCRATIC_QUESTIONING
                new_state['socratic_turns'] = new_state.get('socratic_turns', 0) + 1
                return "", chat_history, new_state, gr.update(visible=False)

            user_intent = classify_user_plan_intent(user_input, problem_summary, new_state)
            if user_intent == 'rejection_doubt':
                new_state['rq2_rejection_count'] = new_state.get('rq2_rejection_count', 0) + 1
                log_interaction(new_state.get('log_file_path'), f"[ì‹œìŠ¤í…œ] RQ2 ê±°ë¶€ íšŸìˆ˜: {new_state['rq2_rejection_count']}", tag="rq2_rejection_count")
                if new_state['rq2_rejection_count'] >= 2:
                    bot_message_content = PROMPTS_EXP["rq2_escalated_rejection_message"]
                    bot_message = call_gpt_api(bot_message_content, user_input, model="gpt-4o-mini", conversation_history=chat_history)
                    chat_history.append({"role": "assistant", "content": bot_message})
                    log_interaction(new_state.get('log_file_path'), f"[ì±—ë´‡]: {bot_message}", tag="rq2_self_decision_escalate_rejection")
                    new_state['stage'] = STAGE_FINAL_PLAN_CONFIRM
                    return "", chat_history, new_state, gr.update(visible=False)
                else:
                    rejection_context = problem_summary
                    rejection_prompt_content = PROMPTS_EXP["rq2_self_decision_rejection_handler"].replace("{{rejection_reason}}", user_input).replace("{{context}}", rejection_context)
                    bot_message = call_gpt_api(rejection_prompt_content, user_input, model="gpt-4o-mini", conversation_history=chat_history)
                    chat_history.append({"role": "assistant", "content": bot_message})
                    log_interaction(new_state.get('log_file_path'), f"[ì±—ë´‡]: {bot_message}", tag="rq2_self_decision_rejected")
                    new_state['stage'] = STAGE_RQ2_PLANNING
                    return "", chat_history, new_state, gr.update(visible=False)
            else:
                new_state['rq2_rejection_count'] = 0
                if "?" in user_input or "ì–´ë–¤ ì¢…ë¥˜" in user_input or "ì–´ë–»ê²Œ" in user_input:
                    followup_prompt_content = PROMPTS_EXP["rq2_self_decision_followup_question"].replace("{{user_input}}", user_input).replace("{{context}}", problem_summary)
                    bot_message = call_gpt_api(followup_prompt_content, user_input, model="gpt-4o-mini", conversation_history=chat_history)
                    chat_history.append({"role": "assistant", "content": bot_message})
                    log_interaction(new_state.get('log_file_path'), f"[ì±—ë´‡]: {bot_message}", tag="rq2_self_decision_followup")
                    new_state['stage'] = STAGE_RQ2_PLANNING
                    return "", chat_history, new_state, gr.update(visible=False)
                else:
                    return _end_session(new_state, chat_history, user_input, user_input, gr.update(visible=False))

        # --- CTRLêµ° ì½”ë“œ ìˆ˜ì • ---
        elif new_state["arm"] == "CTRL":
            prompt_content = PROMPTS_CTRL["rq2_directive_command"].replace("{{predicted_label}}", new_state['predicted_label'])
            bot_message = call_gpt_api(prompt_content, user_input, model="gpt-4o-mini", conversation_history=chat_history)
            
            chat_history.append({"role": "assistant", "content": bot_message})
            log_interaction(new_state.get('log_file_path'), f"[ì±—ë´‡]: {bot_message}", tag="rq2_prompt_directive_no_rag")
            
            new_state['stage'] = STAGE_FINAL_PLAN_CONFIRM
            return "", chat_history, new_state, gr.update(visible=False)
        
    elif current_stage == STAGE_FINAL_PLAN_CONFIRM:
        intent = yn_intent(user_input)
        if intent == "Y":
            final_message = PROMPTS_CTRL["rq2_directive_final_message_accept"]
            new_state['directive_accepts'] += 1
            return _end_session(new_state, chat_history, user_input, final_message, gr.update(visible=False))
        elif intent == "N":
            new_state['directive_denies'] += 1
            
            rejection_prompt_content = PROMPTS_CTRL["rq2_rejection_and_alternative_offer"].replace("{{predicted_label}}", new_state['predicted_label'])
            final_message = call_gpt_api(rejection_prompt_content, user_input, model="gpt-4o-mini", conversation_history=chat_history)
            
            return _end_session(new_state, chat_history, user_input, final_message, gr.update(visible=False))
        else:
            final_message = PROMPTS_CTRL["rq2_directive_final_message_no_intent"]
            return _end_session(new_state, chat_history, user_input, final_message, gr.update(visible=False))

    return "", chat_history, new_state, gr.update(visible=False)

# ==================== Gradio Blocks UI ì„¤ê³„ ====================
with gr.Blocks(css=".gradio-container { max_width: 800px; margin: auto; }") as demo:
    gr.Markdown("# ìˆ˜ë©´ ì¸ì§€ í–‰ë™ ì¹˜ë£Œ ì±—ë´‡ ğŸ’¤")
    state = gr.State({
      'stage': STAGE_NAME_INPUT, 'history': [], 'log_file_path': None,
      'arm': None, 'rq1_mode': None, 'rq2_mode': None, 'rq3_mode': None, 'assignment_seed': None,
      'reasons_count': 0, 'goals_count': 0, 'plan_sentences_count': 0,
      'socratic_turns': 0, 'inform_turns': 0, 'directive_accepts': 0, 'directive_denies': 0,
      'participant_id': None, 'survey_token': None,
      'policy_version': 'v1.0',
      'session_started_at': None,
      'session_ended_at': None,
      'initial_problem_statement': None,
      'socratic_session_initial_input': None,
      'socratic_hints': [],
      'predicted_label': None,
      'rq2_rejection_count': 0,
      'faiss_index_cache': {},
    })
    
    with gr.Row(visible=True) as intro_row:
        name_input = gr.Textbox(label="ë‹‰ë„¤ì„ì„ ì•Œë ¤ì£¼ì„¸ìš”.", placeholder="ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”.")
        name_submit_btn = gr.Button("ì‹œì‘í•˜ê¸°")
    
    chatbot = gr.Chatbot(height=450, type='messages', visible=False)
    msg = gr.Textbox(placeholder="ì—¬ê¸°ì— ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”.", visible=False)
    
    with gr.Row(visible=False) as psychoeducation_row:
        edu_buttons_container = gr.Row()
        with edu_buttons_container:
            for label_key in KOR_LABELS.keys():
                btn = gr.Button(label_key, elem_id=f"edu_btn_{label_key}")
                btn.click(
                    fn=user_input_handler,
                    inputs=[gr.State(label_key), state],
                    outputs=[msg, chatbot, state, psychoeducation_row]
                )
            
            edu_ready_btn = gr.Button("ì¤€ë¹„", elem_id="edu_btn_ready")
            edu_ready_btn.click(
                fn=user_input_handler,
                inputs=[gr.State("ì¤€ë¹„"), state],
                outputs=[msg, chatbot, state, psychoeducation_row]
            )
    
    def start_chat_wrapper(name, state_obj):
        new_state = state_obj.copy()
        new_state['user_name'] = name.strip()
        new_state['participant_id'] = f"pid_{int(time.time())}"
        new_state['session_started_at'] = datetime.now().isoformat()
        new_state['survey_token'] = new_state['participant_id']

        setup_logging(new_state)
        assign_conditions(new_state)
        
        new_state['history'].append({"role": "user", "content": f"ë‹‰ë„¤ì„: {name}"})
        log_interaction(new_state['log_file_path'], f"[ì‚¬ìš©ì]: ë‹‰ë„¤ì„: {name}", tag="user_name_input")
        
        bot_message = f"ë°˜ê°‘ìŠµë‹ˆë‹¤, {new_state['user_name']}ë‹˜!"
        new_state['history'].append({"role": "assistant", "content": bot_message})
        log_interaction(new_state.get('log_file_path'), f"[ì±—ë´‡]: {bot_message}", tag="bot_welcome")
        
        new_state['stage'] = STAGE_PSYCHOEDUCATION_START
        return "", new_state['history'], new_state, gr.update(visible=False), gr.update(visible=True), gr.update(visible=True), gr.update(visible=False)
    
    name_submit_btn.click(
        fn=start_chat_wrapper,
        inputs=[name_input, state],
        outputs=[name_input, chatbot, state, intro_row, msg, chatbot, psychoeducation_row]
    ).then(
        fn=user_input_handler,
        inputs=[gr.State(""), state],
        outputs=[msg, chatbot, state, psychoeducation_row]
    )
    
    msg.submit(
        fn=user_input_handler,
        inputs=[msg, state],
        outputs=[msg, chatbot, state, psychoeducation_row]
    )

if __name__ == "__main__":
    demo.launch(share=True)