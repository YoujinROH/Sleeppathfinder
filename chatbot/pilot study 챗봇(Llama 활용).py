import os
import gradio as gr
import torch
import pandas as pd
import re
import json
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, pipeline
from transformers import RobertaTokenizer, RobertaForSequenceClassification
from sentence_transformers import SentenceTransformer
import faiss
import warnings
import openai
import time
import random
import ast
from peft import PeftConfig, get_peft_model
from huggingface_hub import HfApi
from huggingface_hub.utils import HfHubHTTPError

warnings.filterwarnings("ignore")
DEBUG = True
print("[DEBUG] HF_ACCESS_TOKEN exists?", bool(os.getenv("HF_ACCESS_TOKEN")))

# ========== 0. OpenAI API ì„¤ì • ë° ë²ˆì—­ í•¨ìˆ˜ ==========
openai.api_key = os.getenv("api_key")
if openai.api_key is None:
    raise ValueError("OpenAI API key (api_key) not found in environment variables")

def translate_to_korean(text):
    if DEBUG:
        print("[ë””ë²„ê·¸] translate_to_korean - ì›ë¬¸(ì˜ì–´):", text)
    try:
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": (
                    "You are a professional Korean translator for mental health chatbot dialogues using CBT-I techniques. "
                    "Translate the following English sentence into natural, emotionally supportive Korean that matches the tone of a CBT-I therapy session. "
                    "The translation should sound compassionate yet professional, as if a therapist is speaking in a supportive, non-judgmental manner. "
                    "If the user's sentence is a question, ensure the translation naturally ends with an appropriate question ending in Korean. "
                    "Do not add extra filler or change the meaning. "
                    "Return ONLY the translated Korean sentence."
                )},
                {"role": "user", "content": text}
            ],
            temperature=0.4,
            max_tokens=512
        )
        translated_text = response.choices[0].message.content.strip()
        if DEBUG:
            print("[ë””ë²„ê·¸] translate_to_korean - ë²ˆì—­ ê²°ê³¼(í•œêµ­ì–´):", translated_text)
        return translated_text
    except Exception as e:
        print(f"ğŸš¨ translate_to_korean ì˜¤ë¥˜: {e}")
        return "ì ì‹œ ë²ˆì—­ì— ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

def translate_to_english(text):
    if DEBUG:
        print("[ë””ë²„ê·¸] translate_to_english - ì›ë¬¸(í•œêµ­ì–´):", text)
    try:
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a helpful assistant that translates Korean to English."},
                {"role": "user", "content": f"Translate the following Korean sentence to English:\n\n{text}"}
            ],
            temperature=0.3,
            max_tokens=512
        )
        translated_text = response.choices[0].message.content.strip()
        if DEBUG:
            print("[ë””ë²„ê·¸] translate_to_english - ë²ˆì—­ ê²°ê³¼(ì˜ì–´):", translated_text)
        return translated_text
    except Exception as e:
        print(f"ğŸš¨ translate_to_english ì˜¤ë¥˜: {e}")
        return "ì ì‹œ ë²ˆì—­ì— ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

def normalize_yes_no(user_input):
    normalized = user_input.strip().lower().replace(" ", "")
    if normalized in ["ì˜ˆ", "ë„¤", "ã…‡", "y", "yes", "ì˜ˆ.", "ë„¤."]:
        return "ì˜ˆ"
    elif normalized in ["ì•„ë‹ˆì˜¤", "ì•„ë‹ˆìš”", "ì•„ë‡¨", "ã„´", "n", "no", "ì•„ë‹ˆì˜¤.", "ì•„ë‹ˆìš”."]:
        return "ì•„ë‹ˆì˜¤"
    else:
        return None

def is_valid_concern(text):
    text = text.strip()
    if len(text) < 2:
        return False
    if not re.search(r'[ê°€-í£]', text):
        return False
    return True

# ========== 1. í™˜ê²½ ì„¤ì • ë° ëª¨ë¸ ë¡œë”© ==========
HF_ACCESS_TOKEN = os.getenv('HF_ACCESS_TOKEN')
if HF_ACCESS_TOKEN is None:
    raise ValueError("HF_ACCESS_TOKEN not found in environment variables")

gpu_device = os.environ.get("GPU_DEVICE", "cuda" if torch.cuda.is_available() else "cpu")
llama_device = int(gpu_device.split(":")[1]) if ":" in gpu_device else 0 if gpu_device.startswith("cuda") else -1

# BERT intent classifier
bert_tokenizer = RobertaTokenizer.from_pretrained("youjin129/cbt_i_roberta", use_auth_token=HF_ACCESS_TOKEN)
bert_model = RobertaForSequenceClassification.from_pretrained("youjin129/cbt_i_roberta", use_auth_token=HF_ACCESS_TOKEN)
bert_model.eval().to("cuda" if torch.cuda.is_available() else "cpu")

# LLaMA-3 Pipeline with Fine-Tuning (LoRA) - Private í™˜ê²½ ì ìš©
llama_model_id = "meta-llama/Llama-3.1-8B-Instruct"
lora_ckpt_path = "youjin129/cbt_i_llama3.1_instruct"  # Private repository ì‚¬ìš©

llama_tokenizer = AutoTokenizer.from_pretrained(
    llama_model_id,
    use_auth_token=HF_ACCESS_TOKEN
)
base_llama_model = AutoModelForCausalLM.from_pretrained(
    llama_model_id,
    use_auth_token=HF_ACCESS_TOKEN,
    torch_dtype=torch.float32,
    device_map="auto"
)

peft_config = PeftConfig.from_pretrained(
    lora_ckpt_path,
    use_auth_token=HF_ACCESS_TOKEN
)
lora_llama_model = get_peft_model(base_llama_model, peft_config)
lora_llama_model.eval()

merged_llama_model = lora_llama_model.merge_and_unload()
merged_llama_model.eval()

llama_pipeline = pipeline(
    "text-generation",
    model=merged_llama_model,
    tokenizer=llama_tokenizer,
    device_map="auto"
)
llama_pipeline.model.eval()

embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# ========== 2. ë°ì´í„° ë¡œë”© ==========
file_path = "./data/RAG_0407_eng.xlsx"
data_df = pd.read_excel(file_path)
df = pd.DataFrame([{
    "approach": row["Approach"],
    "user_input": row["User Utterance (English)"],
    "info": row["Therapist Response (English)"]
} for _, row in data_df.iterrows()])

CBT_I_DESCRIPTIONS = {
    "ìˆ˜ë©´ ì œí•œ ìš”ë²• (Sleep Restriction)": (
        "ìˆ˜ë©´ ì œí•œ ìš”ë²•ì€ ì¹¨ëŒ€ì— ë¨¸ë¬´ëŠ” ì‹œê°„ì„ ì˜ë„ì ìœ¼ë¡œ ì¤„ì—¬, ì¹¨ëŒ€ì™€ ìˆ˜ë©´ ì‚¬ì´ì˜ ì˜¬ë°”ë¥¸ ì—°ê²°ê³ ë¦¬ë¥¼ ì¬êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. "
        "ì˜ˆë¥¼ ë“¤ì–´, ì¹¨ëŒ€ì— 10ì‹œê°„ ë¨¸ë¬¼ì§€ë§Œ ì‹¤ì œ ìˆ˜ë©´ ì‹œê°„ì´ 5ì‹œê°„ì¸ ê²½ìš°, ì²˜ìŒì—ëŠ” 5ì‹œê°„ë§Œ ì¹¨ëŒ€ì—ì„œ ìê³  ì ì°¨ ì‹œê°„ì„ ëŠ˜ë ¤ê°€ë©´ì„œ ëª¸ì´ ì¹¨ëŒ€ë¥¼ â€˜ìˆ™ë©´ì„ ìœ„í•œ ì¥ì†Œâ€™ë¡œ ì¸ì‹í•˜ë„ë¡ ë•ìŠµë‹ˆë‹¤."
    ),
    "ìê·¹ ì¡°ì ˆ ìš”ë²• (Stimulus Control)": (
        "ìê·¹ ì¡°ì ˆ ìš”ë²•ì€ ì¹¨ëŒ€ì™€ ìˆ˜ë©´ì˜ ê´€ê³„ë¥¼ ì¬ì •ë¦½í•˜ì—¬, ì¹¨ëŒ€ë¥¼ ì˜¤ì§ ìˆ˜ë©´ë§Œì„ ìœ„í•œ ì¥ì†Œë¡œ ì¸ì‹í•˜ê²Œ ë§Œë“œëŠ” ì¹˜ë£Œë²•ì…ë‹ˆë‹¤. "
        "ì ì´ ì˜¤ì§€ ì•Šì„ ë•ŒëŠ” ì¦‰ì‹œ ì¹¨ëŒ€ì—ì„œ ë²—ì–´ë‚˜ê³ , ì¹¨ëŒ€ì—ì„œëŠ” ì˜¤ì§ ìˆ˜ë©´ë§Œ ì·¨í•˜ëŠ” ìŠµê´€ì„ ê¸°ë¥´ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤."
    ),
    "ìˆ˜ë©´ ìœ„ìƒ êµìœ¡ (Sleep Hygiene)": (
        "ìˆ˜ë©´ ìœ„ìƒ êµìœ¡ì€ ê±´ê°•í•œ ìˆ˜ë©´ì„ ìœ„í•´ ìƒí™œ ìŠµê´€ì„ ê°œì„ í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. "
        "ì˜ˆë¥¼ ë“¤ì–´, ë‚®ì—ëŠ” ì¹´í˜ì¸ ì„­ì·¨ë¥¼ ì¤„ì´ê³ , ì¼ì •í•œ ì‹œê°„ì— ìê³  ì¼ì–´ë‚˜ë©°, ì·¨ì¹¨ ì „ì—ëŠ” ì „ìê¸°ê¸° ì‚¬ìš©ê³¼ ë°ì€ ì¡°ëª…ì„ í”¼í•˜ê³ , ì§€ë‚˜ì¹˜ê²Œ ëŠ¦ì€ ì‹œê°„ì˜ ìš´ë™ì„ ì‚¼ê°€ëŠ” ë“±ì˜ ìŠµê´€ì„ í¬í•¨í•©ë‹ˆë‹¤."
    ),
    "ì´ì™„ ìš”ë²• (Relaxation Techniques)": (
        "ì´ì™„ ìš”ë²•ì€ ì‹¬ë¦¬ì Â·ì‹ ì²´ì  ê¸´ì¥ì„ ì™„í™”ì‹œì¼œ ìì—°ìŠ¤ëŸ¬ìš´ ìˆ˜ë©´ì„ ìœ ë„í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. "
        "ì‹¬í˜¸í¡, ëª…ìƒ, ê°€ë²¼ìš´ ìŠ¤íŠ¸ë ˆì¹­, ê·¸ë¦¬ê³  ê·¼ìœ¡ ì´ì™„ ìš´ë™ì„ í†µí•´ ëª¸ê³¼ ë§ˆìŒì„ í¸ì•ˆí•˜ê²Œ ë§Œë“œëŠ” ê²ƒì´ ì£¼ëœ ëª©í‘œì…ë‹ˆë‹¤."
    ),
    "ì¸ì§€ì  ì¬êµ¬ì„± (Cognitive Restructuring)": (
        "ì¸ì§€ì  ì¬êµ¬ì„±ì€ ìˆ˜ë©´ê³¼ ê´€ë ¨ëœ ë¶€ì •ì ì¸ ìƒê°ì´ë‚˜ ê±±ì •ì„ ê¸ì •ì ìœ¼ë¡œ ì „í™˜í•˜ëŠ” ì¹˜ë£Œë²•ì…ë‹ˆë‹¤. "
        "ì˜ˆë¥¼ ë“¤ì–´, 'ì˜¤ëŠ˜ë„ ì ì„ ëª» ìë©´ í°ì¼ ë‚  ê±°ì•¼'ë¼ëŠ” ë¹„ê´€ì ì¸ ìƒê° ëŒ€ì‹ , 'ì¡°ê¸ˆ ë¶€ì¡±í•˜ë”ë¼ë„ ëª¸ì€ ì ì°¨ ì ì‘í•  ìˆ˜ ìˆì–´'ë¼ëŠ” ê¸ì •ì ì¸ ê´€ì ìœ¼ë¡œ ë³€í™”ì‹œí‚¤ë©°, ê±±ì • ê´€ë¦¬ì™€ ê°ì • ì¡°ì ˆ, í–‰ë™ ì‹¤í—˜ ë“±ì˜ ê¸°ë²•ì„ í†µí•´ ë¶ˆë©´ì¦ìœ¼ë¡œ ì¸í•œ ë¶ˆì•ˆì„ ì¤„ì´ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤."
    )
}

ENG_TO_KOR_KEY = {
    "Sleep Restriction": "ìˆ˜ë©´ ì œí•œ ìš”ë²• (Sleep Restriction)",
    "Stimulus Control": "ìê·¹ ì¡°ì ˆ ìš”ë²• (Stimulus Control)",
    "Sleep Hygiene": "ìˆ˜ë©´ ìœ„ìƒ êµìœ¡ (Sleep Hygiene)",
    "Relaxation Techniques": "ì´ì™„ ìš”ë²• (Relaxation Techniques)",
    "Cognitive Restructuring": "ì¸ì§€ì  ì¬êµ¬ì„± (Cognitive Restructuring)"
}
KOR_TO_ENG_KEY = {v: k for k, v in ENG_TO_KOR_KEY.items()}

# ========== 3. ì´ˆê¸° ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬ ==========
def get_initial_state():
    return {
        "user_name": None,
        "history": [],
        "prev_approaches": [],
        "recommended_approach": None,
        "consult_query": None,
        "faiss_index_cache": {},
        "mode": "í•™ìŠµ ëª¨ë“œ",  # ì´ˆê¸°ì—ëŠ” ë°˜ë“œì‹œ í•™ìŠµ ëª¨ë“œ
        "consulting_active": False,
        "socratic_active": False,
        "socratic_depth": 0,
        "max_depth": 5,
        "socratic_hints": [],
        "current_subquestion": None,
        "current_confidence": "low",
        "current_type": None,
        "self_decision_pending": False,
        "technique_selection_pending": False,
        "waiting_end_confirmation": False,
        "awaiting_termination": False,
        "learning_index": 0,
        "iterative_advice_active": False,
        "iterative_context": "",
        "current_iterative_advice": "",
        "type_history": []
    }

def reset_state(state):
    # ì‚¬ìš©ì ì´ë¦„ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³  ë‚˜ë¨¸ì§€ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•¨
    user_name = state.get("user_name")
    new_state = get_initial_state()
    new_state["user_name"] = user_name
    return new_state

# ------------------------------
# 4. ì£¼ìš” í•¨ìˆ˜ë“¤ (stateë¥¼ ì¸ìë¡œ ë°›ë„ë¡ ìˆ˜ì •)
# ------------------------------

def extract_type_flexible(text):
    match = re.search(r'Type:\s*(\w+)', text)
    if match:
        t = match.group(1).lower()
        valid_types = ["clarity", "implication_consequences", "reasons_evidence", "assumptions", "alternate_viewpoints_perspectives"]
        if t in valid_types:
            return t
    return None

def classify_intent_with_bert(user_input):
    inputs = bert_tokenizer(user_input, return_tensors="pt", padding=True, truncation=True, max_length=128)
    inputs = {k: v.to(bert_model.device) for k, v in inputs.items()}
    with torch.no_grad():
        logits = bert_model(**inputs).logits
    predicted_idx = torch.argmax(logits, dim=1).item()
    result = list(ENG_TO_KOR_KEY.keys())[predicted_idx]
    if DEBUG:
        print("[ë””ë²„ê·¸] classify_intent_with_bert - Predicted technique (English):", result)
    return result

def retrieve_info_by_approach(query, approach, df, state, top_k=3):
    if approach not in state["faiss_index_cache"]:
        filtered_df = df[df["approach"] == approach]
        if filtered_df.empty:
            raise ValueError(f"No data found for approach: {approach}")
        input_list = filtered_df["user_input"].dropna().tolist()
        emb = embedding_model.encode(input_list, convert_to_tensor=True)
        index = faiss.IndexFlatL2(emb.shape[1])
        index.add(emb.cpu().numpy())
        state["faiss_index_cache"][approach] = (index, filtered_df)
    index, filtered_df = state["faiss_index_cache"][approach]
    query_vector = embedding_model.encode([query], convert_to_tensor=True).cpu().numpy()
    _, indices = index.search(query_vector, top_k)
    if DEBUG:
        print("[ë””ë²„ê·¸] retrieve_info_by_approach - Retrieved indices:", indices)
    return filtered_df.iloc[indices[0][0]]['info']

def merge_hints_to_utterance(hints):
    return " ".join([f'For the question "{q}", the answer was "{a}".' for q, a in hints])

def postprocess_response(text: str) -> str:
    clean = re.sub(r'\s+', ' ', text).strip()
    if not re.search(r'[.!?]"?$', clean):
        clean += "."
    return clean

def generate_confidence_field(user_input, context, depth):
    conf_prompt = f"""You are a CBT-I assistant.
Evaluate your confidence in understanding the user's situation.
User's statement: "{user_input}"
Context: "{context}"
Depth: {depth}
Respond ONLY with one word: low, middle, or high.
"""
    out = llama_pipeline(conf_prompt, max_new_tokens=10, temperature=0.4, top_p=0.9)[0]['generated_text']
    conf = out.strip().lower()
    if conf not in ["low", "middle", "high"]:
        conf = "low"
    return conf

def generate_type_field(user_input, context, depth, state):
    type_prompt = f"""
You are a Socratic Question Classifier for CBT-I sleep therapy chatbot.
Below are 5 Socratic Question Types based on Paul & Elder (2019), each with a description and example:
1) clarity
   - Probes unclear or vague thoughts.
   - Example: "What do you mean by that?"
2) assumptions
   - Probes hidden assumptions or beliefs.
   - Example: "What assumptions are you making?"
3) reasons_evidence
   - Probes the reasoning or evidence behind a claim.
   - Example: "What makes you think this is true?"
4) implication_consequences
   - Probes what might happen next.
   - Example: "What do you think will happen if this continues?"
5) alternate_viewpoints_perspectives
   - Probes different angles or perspectives.
   - Example: "Is there another way to look at this?"
--------------------------------------------------
Now classify the userâ€™s statement:
User: "{user_input}"
Context: "{context}"
Respond in the following format only:
Type: <one of clarity, assumptions, reasons_evidence, implication_consequences, alternate_viewpoints_perspectives>
""".strip()
    try:
        out = llama_pipeline(type_prompt, max_new_tokens=60, temperature=0.3, top_p=0.8)[0]["generated_text"]
        if DEBUG:
            print("[ë””ë²„ê·¸] generate_type_field - Raw output:", out)
        typ = extract_type_flexible(out)
        if typ is None:
            typ = "clarity"

        if "type_history" not in state:
            state["type_history"] = []
        state["type_history"].append(typ)
        if len(state["type_history"]) > 3:
            state["type_history"].pop(0)

        if state["type_history"].count("clarity") >= 2:
            typ = random.choice(["assumptions", "reasons_evidence", "implication_consequences", "alternate_viewpoints_perspectives"])
            if DEBUG:
                print(f"[ë””ë²„ê·¸] clarity ê³¼ë‹¤ íƒì§€ - ê°•ì œ ì „í™˜: {typ}")
        return typ
    except Exception as e:
        print(f"ğŸš¨ generate_type_field ì˜¤ë¥˜: {e}")
        return "clarity"

def generate_acknowledgment(user_input, context, depth):
    ack_prompt = f"""
You are a compassionate CBT-I assistant. 
The user is sharing their sleep-related struggles.
User statement: "{user_input}"
Conversation context: "{context}"
Depth: {depth}
Instructions:
1. Generate a brief but emotionally nuanced empathetic acknowledgment.
2. Reflect the *specific emotional tone* of the user's message (e.g., frustration, sadness, anxiety, exhaustion).
3. Avoid generic phrases like "I understand" or "Your feelings are valid."
4. Use diverse expressions of empathy that sound natural and human.
5. You may use metaphors, imagery, or personal-style expressions to show genuine care.
Examples:
- "That must be incredibly draining to go through every night."
- "It sounds like you're carrying a lot of stress, and that's completely understandable."
- "I can really feel how upsetting this has been for you."
- "You're doing your best, and this sounds tougher than most people realize."
Respond ONLY with the empathetic sentence.
"""
    out = llama_pipeline(ack_prompt, max_new_tokens=60, temperature=0.5, top_p=0.9)[0]['generated_text']
    ack = out.strip()
    return translate_to_korean(ack)

def generate_subquestion_field(user_input, context, depth, typ):
    prompt = llama_pipeline.tokenizer.apply_chat_template([
        {"role": "system", "content": (
            "You are a Socratic therapist helping a user struggling with sleep problems."
            " Your job is to generate ONE Socratic follow-up question that matches the given type,"
            " but also feels natural, emotionally supportive, and contextually appropriate."
            " Do not be abstract. Sound like a real therapist helping someone who feels anxious, overwhelmed, or restless."
        )},
        {"role": "user", "content": f"""
User's concern: "{user_input}"
Previous conversation context: "{context}"
Socratic question type: {typ}
Instructions:
- Only generate ONE natural, emotionally grounded question.
- Avoid vague or philosophical questions.
- Ask a specific and supportive question that could realistically come from a CBT-I therapist.
- Return ONLY the English question. No quotes, no explanations.
"""}
    ], tokenize=False, add_generation_prompt=True)

    try:
        result = llama_pipeline(prompt, max_new_tokens=60, temperature=0.4, top_p=0.9)[0]['generated_text']
        question = result[len(prompt):].strip()
        if "?" not in question or len(question) < 5 or any(bad in question.lower() for bad in [
            "accomplish", "goal", "trying to", "how do you feel", "purpose"
        ]):
            fallback = {
                "clarity": "Could you tell me more about what that feels like?",
                "assumptions": "What might you be assuming when that thought comes up?",
                "reasons_evidence": "What makes you think that will happen?",
                "implication_consequences": "What do you think might happen if this continues?",
                "alternate_viewpoints_perspectives": "Is there another way you could view this situation?"
            }
            question = fallback.get(typ, "Could you explain a bit more about that?")
        return question
    except Exception as e:
        print(f"ğŸš¨ generate_subquestion_field ì˜¤ë¥˜: {e}")
        return "Could you explain a bit more about that?"

def generate_full_subquestion_v2(user_input, context="", depth=0, state=None):
    conf = generate_confidence_field(user_input, context, depth)
    typ = generate_type_field(user_input, context, depth, state)
    subq = generate_subquestion_field(user_input, context, depth, typ)
    subq_kr = translate_to_korean(subq)
    ack = generate_acknowledgment(user_input, context, depth)
    state["current_confidence"] = conf
    state["current_type"] = typ
    final_output = f"{ack.strip()} í˜¹ì‹œ {subq_kr.strip().rstrip('.').rstrip('?')}?"
    if DEBUG:
        print("[ë””ë²„ê·¸] generate_full_subquestion_v2 - confidence:", conf)
        print("[ë””ë²„ê·¸] generate_full_subquestion_v2 - type:", typ)
        print("[ë””ë²„ê·¸] generate_full_subquestion_v2 - subquestion:", subq)
        print("[ë””ë²„ê·¸] generate_full_subquestion_v2 - subquestion (KR):", subq_kr)
        print("[ë””ë²„ê·¸] generate_full_subquestion_v2 - acknowledgment:", ack)
        print("[ë””ë²„ê·¸] generate_full_subquestion_v2 - ìµœì¢… ì¶œë ¥:", final_output)
    return postprocess_response(final_output)

def generate_response(user_input, approach_en, context, include_termination=True):
    kor_key = ENG_TO_KOR_KEY.get(approach_en)
    desc_kr = CBT_I_DESCRIPTIONS.get(kor_key, "")
    sentences = re.split(r'(?<=[.!?])\s+', context)
    summary = ' '.join(sentences[:2])
    prompt = llama_pipeline.tokenizer.apply_chat_template([
        {"role": "system", "content": (
            "You are a warm and empathetic CBT-I therapist. Respond with a natural, conversational answer in Korean. "
            "Please express empathy and provide practical CBT advice based on the selected technique. "
            "Do not use numbered lists or bullet points."
        )},
        {"role": "user", "content": f"""
User concern: {user_input}
Recommended CBT-I technique: {approach_en}
CBT-I Description: {desc_kr}
Extra context: {summary}
Please return your response in Korean.
If include_termination is True, end your answer with a polite question asking if the user wants to end the session.
Otherwise, just end your response naturally.
"""}
    ], tokenize=False, add_generation_prompt=True)
    out = llama_pipeline(prompt, max_new_tokens=180, temperature=0.4, top_p=0.8)[0]['generated_text']
    english_response = re.sub(r'\s+', ' ', out[len(prompt):].strip())
    korean_response = translate_to_korean(english_response)
    return postprocess_response(korean_response)

def generate_self_decision_message(state):
    combined = state["consult_query"] + " " + merge_hints_to_utterance(state["socratic_hints"])
    approach_en = classify_intent_with_bert(combined).strip()
    state["recommended_approach"] = approach_en
    kor_key = ENG_TO_KOR_KEY.get(approach_en)
    desc_kr = CBT_I_DESCRIPTIONS.get(kor_key, "")
    prompt = llama_pipeline.tokenizer.apply_chat_template([
        {"role": "system", "content": "You are a CBT-I expert who provides natural, empathetic recommendations based on user concerns."},
        {"role": "user", "content": f"""
The user is experiencing sleep issues and has provided the following concern and dialogue history:
Concern: {combined}
Based on this, you recommend the CBT-I technique: "{kor_key}" ({approach_en}).
Here is a brief explanation of the technique in Korean: "{desc_kr}"
Please:
- Begin with an empathetic statement
- Clearly explain why this specific technique could be helpful
- End your response with a polite question asking if the user would like to try this method.
Return your answer in natural Korean.
"""}
    ], tokenize=False, add_generation_prompt=True)
    
    raw = llama_pipeline(prompt, max_new_tokens=180, temperature=0.4, top_p=0.8)[0]['generated_text']
    english_reply = re.sub(r'\s+', ' ', raw[len(prompt):].strip())
    korean_reply = translate_to_korean(english_reply)

    # âœ… ì—¬ê¸° ìˆ˜ì •
    termination_kor = "ì´ ë°©ë²•ì„ ì‹œë„í•´ë³´ì‹œê² ì–´ìš”? 'ì˜ˆ' ë˜ëŠ” 'ì•„ë‹ˆì˜¤'ë¡œ ë‹µí•´ì£¼ì„¸ìš”."
    if not korean_reply.strip().endswith(termination_kor):
        korean_reply += "\n\n" + termination_kor  # ì¤„ë°”ê¿ˆ ì¶”ê°€ë¡œ ë§í’ì„  ìœ„ í´ë¦¬í•‘ ë°©ì§€

    return postprocess_response(korean_reply)

def generate_personalized_advice(user_input, last_advice, technique_name):
    prompt = llama_pipeline.tokenizer.apply_chat_template([
        {"role": "system", "content": (
            "You are a CBT-I sleep therapist. The user has already received advice about a specific CBT-I technique, "
            "and is now asking a follow-up question or sharing a concern related to applying it. "
            "Respond in a natural, conversational tone with personalized advice that is warm and supportive. "
            "Do not use numbered lists or bullet points."
        )},
        {"role": "user", "content": f"""
CBT-I technique: {technique_name}
Previous advice given: "{last_advice}"
User's follow-up message:
"{user_input}"
Please respond in Korean in a natural manner without repeating the entire explanation of the technique.
"""}
    ], tokenize=False, add_generation_prompt=True)
    output = llama_pipeline(prompt, max_new_tokens=180, temperature=0.5, top_p=0.9)[0]['generated_text']
    return postprocess_response(translate_to_korean(output[len(prompt):].strip()))

def finalize_socratic_and_advice(state):
    merged = merge_hints_to_utterance(state["socratic_hints"])
    query = state["consult_query"] + " " + merged
    approach_en = state["recommended_approach"]
    if not approach_en:
        raise ValueError("No recommended_approach found.")
    info = retrieve_info_by_approach(query, approach_en, df, state)
    korean_response = generate_response(query, approach_en, info, include_termination=False)
    return postprocess_response(korean_response + " ì¶”ê°€ ì˜ê²¬ì´ ìˆìœ¼ì‹œë©´ ì…ë ¥í•´ ì£¼ì„¸ìš”. ë§Œì¡±í•˜ì‹œë©´ 'ë§Œì¡±'ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")

def save_history_to_json(state):
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ Hugging Face Token ë¶ˆëŸ¬ì˜¤ê¸° (ê¸°ì¡´ ë°©ì‹ê³¼ ë™ì¼)
    HF_ACCESS_TOKEN = os.getenv("HF_ACCESS_TOKEN")
    if HF_ACCESS_TOKEN is None:
        raise ValueError("HF_ACCESS_TOKEN í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # 1. ë¡œì»¬ì— ì €ì¥
    os.makedirs("./history", exist_ok=True)
    filename = f"./history/{state['user_name']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(state["history"], f, ensure_ascii=False, indent=4)

    # 2. Hugging Face Hubì— ì—…ë¡œë“œ
    try:
        api = HfApi(token=HF_ACCESS_TOKEN)  # âœ… ì—¬ê¸° í•µì‹¬

        repo_id = "youjin129/cbt-i-history"
        repo_type = "dataset"
        path_in_repo = f"history/{os.path.basename(filename)}"

        # ë¦¬í¬ì§€í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        try:
            api.create_repo(repo_id=repo_id, repo_type=repo_type, private=False)
        except HfHubHTTPError as e:
            if e.response.status_code == 409:
                print(f"[INFO] ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë¦¬í¬ì§€í† ë¦¬ì…ë‹ˆë‹¤: {repo_id}")
            else:
                raise

        # ì—…ë¡œë“œ
        api.upload_file(
            path_or_fileobj=filename,
            path_in_repo=path_in_repo,
            repo_id=repo_id,
            repo_type=repo_type
        )
        upload_msg = "âœ… ëŒ€í™” ê¸°ë¡ì´ Hugging Face Hubì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤."
    except Exception as e:
        upload_msg = f"âŒ ëŒ€í™” ê¸°ë¡ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}"

    return f"ğŸ’¾ ëŒ€í™” ê¸°ë¡ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filename}\n{upload_msg}"

# ------------------------------
# 5. ìƒë‹´/í•™ìŠµ ëª¨ë“œ ì²˜ë¦¬ í•¨ìˆ˜ (state ì¸ì ì¶”ê°€)
# ------------------------------

def process_learning_mode(user_input, state):
    # ì´ˆê¸° í•™ìŠµ ëª¨ë“œ ì‹œì‘ ì‹œ, "ì˜ˆ"ê°€ ì•„ë‹ˆë¼ë©´ ì„¤ë“ ë©”ì‹œì§€ ì¶œë ¥ í›„ ë‹¤ì‹œ ë¬¼ì–´ë´„
    if state.get("learning_index", 0) == 0:
        if normalize_yes_no(user_input) != "ì˜ˆ":
            state["history"].append((None, "ìˆ˜ë©´ ê°œì„  ê¸°ë²•ì— ê´€í•œ ì„¤ëª…ì€ ë§¤ìš° ìœ ìµí•©ë‹ˆë‹¤. ì œê°€ ì•Œë ¤ë“œë¦´ ê¸°ë²•ë“¤ì´ í° ë„ì›€ì´ ë  ê±°ì˜ˆìš”. 'ì˜ˆ'ë¼ê³  ì…ë ¥í•´ ì£¼ì‹œë©´ ì‹œì‘í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."))
            return state["history"], state
    
    if user_input.strip() != "":
        idx = state.get("learning_index", 0)
        if idx < len(TECHNIQUES_ORDER):
            technique = TECHNIQUES_ORDER[idx]
            explanation = CBT_I_DESCRIPTIONS.get(technique, "ì„¤ëª…ì´ ì—†ìŠµë‹ˆë‹¤.")
            state["learning_index"] = idx + 1
            if state["learning_index"] < len(TECHNIQUES_ORDER):
                msg = (f"[{technique}]\n\n{explanation}\n\n"
                       "ë‹¤ìŒ ê¸°ë²•ìœ¼ë¡œ ë„˜ì–´ê°€ê¸° ìœ„í•´ ì•„ë¬´ ë‚´ìš©ì´ë‚˜ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
            else:
                msg = (f"[{technique}]\n\n{explanation}\n\n"
                       "ëª¨ë“  ê¸°ë²• í•™ìŠµì„ ë§ˆì³¤ìŠµë‹ˆë‹¤. ì´ì œ ìƒë‹´ ëª¨ë“œë¡œ ì „í™˜í•´ë„ ë˜ê² ìŠµë‹ˆê¹Œ?ğŸ˜Š ì›í•˜ì‹ ë‹¤ë©´ ìˆ˜ë©´ì— ëŒ€í•œ ê³ ë¯¼ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                state["mode"] = "ìƒë‹´ ëª¨ë“œ"
            state["history"].append((None, msg))
            return state["history"], state
        else:
            state["mode"] = "ìƒë‹´ ëª¨ë“œ"
            state["history"].append((None, "ëª¨ë“  ê¸°ë²• í•™ìŠµì„ ë§ˆì³¤ìŠµë‹ˆë‹¤. ì´ì œ ìƒë‹´ ëª¨ë“œë¡œ ì „í™˜í•´ë„ ë˜ê² ìŠµë‹ˆê¹Œ?ğŸ˜Š ì›í•˜ì‹ ë‹¤ë©´ ìˆ˜ë©´ì— ëŒ€í•œ ê³ ë¯¼ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."))
            return state["history"], state
    else:
        state["history"].append((None, "ì•„ë¬´ ë‚´ìš©ì´ë¼ë„ ì…ë ¥í•´ ì£¼ì„¸ìš”."))
        return state["history"], state

def consult_mode(user_input, state):
    chat = state["history"]
    
    # ìƒë‹´ ëª¨ë“œ ì´ˆê¸° ì§„ì… ì‹œ, ì…ë ¥ì´ ì¶©ë¶„í•œ ê³ ë¯¼ì¸ì§€ í™•ì¸ (ì˜ˆ: "ã…‡", ".", "ã„¹" ë“±ì€ ë¶€ì í•©)
    if not state.get("consulting_active"):
        if not is_valid_concern(user_input):
            chat.append((None, "ìˆ˜ë©´ì— ëŒ€í•´ ì¡°ê¸ˆ ë” êµ¬ì²´ì ì¸ ê³ ë¯¼ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."))
            return chat, state
        else:
            eng_input = translate_to_english(user_input)
            state.update({
                "consulting_active": True,
                "socratic_active": True,
                "consult_query": eng_input,
                "socratic_depth": 0,
                "socratic_hints": []
            })
            subq = generate_full_subquestion_v2(eng_input, depth=0, state=state)
            state["current_subquestion"] = subq
            chat.append((user_input, None))
            chat.append((None, subq))
            return chat, state

    # ì¶”ê°€ ê°œì„  1): ì§„í–‰ ì¤‘ì—ë„ ë¬´ì˜ë¯¸í•œ ì…ë ¥(ì˜ˆ: í•œ ê¸€ì ë“±)ì„ ê±°ë¶€í•˜ê³  êµ¬ì²´ì  ê³ ë¯¼ì„ ì¬ìš”êµ¬
    if not is_valid_concern(user_input) and normalize_yes_no(user_input) is None:
        chat.append((None, "ì…ë ¥ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ìˆ˜ë©´ ë¬¸ì œì— ëŒ€í•´ êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì„¸ìš”."))
        return chat, state

    # 0. iterative advice ì§„í–‰ ì¤‘ì´ë©´ ë¨¼ì € ì²˜ë¦¬
    if state.get("iterative_advice_active", False):
        feedback = user_input.strip()
        chat.append((user_input, None))
        if feedback.lower() == "ë§Œì¡±":
            final_advice = state.get("current_iterative_advice", "")
            state["iterative_advice_active"] = False
            state["awaiting_termination"] = True
            chat.append((None, "ëŒ€í™”ë¥¼ ì¢…ë£Œí•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? 'ì˜ˆ' ë˜ëŠ” 'ì•„ë‹ˆì˜¤'ë¡œ ë‹µí•´ì£¼ì„¸ìš”."))
            return chat, state
        else:
            state["iterative_context"] += " " + feedback
            last_advice = state.get("current_iterative_advice", "")
            tech_name = ENG_TO_KOR_KEY.get(state["recommended_approach"], "ìˆ˜ë©´ ê¸°ë²•")
            new_advice = generate_personalized_advice(feedback, last_advice, tech_name)
            state["current_iterative_advice"] = new_advice
            chat.append((None, new_advice + "\nì¶”ê°€ ì˜ê²¬ì´ ìˆìœ¼ì‹œë©´ ì…ë ¥í•´ ì£¼ì„¸ìš”. ë§Œì¡±í•˜ì‹œë©´ 'ë§Œì¡±'ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."))
            return chat, state

    # 1. ì¢…ë£Œ ì²˜ë¦¬
    if state.get("awaiting_termination", False):
        ans = user_input.strip().lower()
        chat.append((user_input, None))
        if ans == "ì˜ˆ":
            msg = save_history_to_json(state)
            chat.append((None, f"ê°ì‚¬í•©ë‹ˆë‹¤. ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. {msg}"))
            state = reset_state(state)
            return chat, state
        elif ans == "ì•„ë‹ˆì˜¤":
            chat.append((None, "ëŒ€í™”ë¥¼ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤. ì–´ë–¤ ê³ ë¯¼ì´ ìˆìœ¼ì‹ ê°€ìš”?"))
            state["awaiting_termination"] = False
            return chat, state
        else:
            chat.append((None, "ëŒ€í™”ë¥¼ ì¢…ë£Œí•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? 'ì˜ˆ' ë˜ëŠ” 'ì•„ë‹ˆì˜¤'ë¡œ ë‹µí•´ì£¼ì„¸ìš”."))
            return chat, state

    if user_input.strip().lower() in ["exit"]:
        msg = save_history_to_json(state)
        chat.append((None, f"ëŒ€í™”ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. {msg} ë‹¤ì‹œ ì‹œì‘í•˜ê³  ì‹¶ìœ¼ì‹œë©´ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."))
        state = reset_state(state)
        return chat, state

    if state.get("waiting_end_confirmation", False):
        ans = user_input.strip().lower()
        if ans == "ì˜ˆ":
            msg = save_history_to_json(state)
            chat.append((None, f"ê°ì‚¬í•©ë‹ˆë‹¤. ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. {msg}"))
            state = reset_state(state)
            return chat, state
        elif ans == "ì•„ë‹ˆì˜¤":
            chat.append((None, "ëŒ€í™”ë¥¼ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤. ì–´ë–¤ ê³ ë¯¼ì´ ìˆìœ¼ì‹ ê°€ìš”?"))
            state["waiting_end_confirmation"] = False
            return chat, state
        else:
            chat.append((None, "ëŒ€í™”ë¥¼ ì¢…ë£Œí•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? 'ì˜ˆ' ë˜ëŠ” 'ì•„ë‹ˆì˜¤'ë¡œ ë‹µí•´ì£¼ì„¸ìš”."))
            return chat, state

    # 2. Self-decision ë‹¨ê³„
    if state.get("self_decision_pending", False):
        ans = user_input.strip().lower()
        if ans == "ì˜ˆ":
            state["iterative_advice_active"] = True
            state["iterative_context"] = state["consult_query"] + " " + merge_hints_to_utterance(state["socratic_hints"])
            info = retrieve_info_by_approach(state["iterative_context"], state["recommended_approach"], df, state)
            initial_advice = generate_response(state["iterative_context"], state["recommended_approach"], info)
            state["current_iterative_advice"] = initial_advice
            chat.append((None, initial_advice + "\nì¶”ê°€ ì˜ê²¬ì´ ìˆìœ¼ì‹œë©´ ì…ë ¥í•´ ì£¼ì„¸ìš”. ë§Œì¡±í•˜ì‹œë©´ 'ë§Œì¡±'ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."))
            state["self_decision_pending"] = False
            return chat, state
        elif ans == "ì•„ë‹ˆì˜¤":
            response_text = ("ì•Œê² ìŠµë‹ˆë‹¤. ëŒ€ì‹ , ì•„ë˜ ê¸°ë²•ë“¤ ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ê¸° ì›í•˜ì‹ ë‹¤ë©´ í•´ë‹¹ë˜ëŠ” ë²ˆí˜¸(1~5)ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”:\n"
                             "1. ìˆ˜ë©´ ì œí•œ ìš”ë²• (Sleep Restriction)\n"
                             "2. ìê·¹ ì¡°ì ˆ ìš”ë²• (Stimulus Control)\n"
                             "3. ìˆ˜ë©´ ìœ„ìƒ êµìœ¡ (Sleep Hygiene)\n"
                             "4. ì´ì™„ ìš”ë²• (Relaxation Techniques)\n"
                             "5. ì¸ì§€ì  ì¬êµ¬ì„± (Cognitive Restructuring)")
            chat.append((None, response_text))
            state["self_decision_pending"] = False
            state["technique_selection_pending"] = True
            return chat, state
        else:
            chat.append((None, "â€˜ì˜ˆâ€™ ë˜ëŠ” â€˜ì•„ë‹ˆì˜¤â€™ë¡œ ë‹µí•´ ì£¼ì„¸ìš”."))
            return chat, state

    # 3. Technique ì„ íƒ ë‹¨ê³„ (ë²ˆí˜¸ ì…ë ¥ ì‹œ ëì— ì (.) ë¶™ì€ ê²½ìš°ë„ ì²˜ë¦¬)
    if state.get("technique_selection_pending", False):
        # ë²ˆí˜¸ ë’¤ì— ìˆëŠ” '.' ì œê±°í•˜ê³  ë¹„êµ
        normalized_ans = re.sub(r'\.$', '', user_input.strip())
        if normalized_ans in ["1", "2", "3", "4", "5"]:
            matched_key = list(ENG_TO_KOR_KEY.keys())[int(normalized_ans)-1]
            state["recommended_approach"] = matched_key
            state["iterative_advice_active"] = True
            state["iterative_context"] = state["consult_query"] + " " + merge_hints_to_utterance(state["socratic_hints"])
            info = retrieve_info_by_approach(state["iterative_context"], state["recommended_approach"], df, state)
            initial_advice = generate_response(state["iterative_context"], state["recommended_approach"], info, include_termination=False)
            state["current_iterative_advice"] = initial_advice
            chat.append((None, initial_advice + "\nì¶”ê°€ ì˜ê²¬ì´ ìˆìœ¼ì‹œë©´ ì…ë ¥í•´ ì£¼ì„¸ìš”. ë§Œì¡±í•˜ì‹œë©´ 'ë§Œì¡±'ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."))
            state["technique_selection_pending"] = False
            return chat, state
        else:
            chat.append((None, "ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤. ì•„ë˜ ë²ˆí˜¸ ì¤‘ í•˜ë‚˜ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”:\n"
                               "1. ìˆ˜ë©´ ì œí•œ ìš”ë²• (Sleep Restriction)\n"
                               "2. ìê·¹ ì¡°ì ˆ ìš”ë²• (Stimulus Control)\n"
                               "3. ìˆ˜ë©´ ìœ„ìƒ êµìœ¡ (Sleep Hygiene)\n"
                               "4. ì´ì™„ ìš”ë²• (Relaxation Techniques)\n"
                               "5. ì¸ì§€ì  ì¬êµ¬ì„± (Cognitive Restructuring)"))
            return chat, state

    # 4. ì´ˆê¸° ìƒë‹´ ì‹œì‘ (ìƒë‹´ ëª¨ë“œë¡œ ì „í™˜ í›„ ìµœì´ˆ ìƒë‹´)
    if not state["consulting_active"]:
        eng_input = translate_to_english(user_input)
        state.update({
            "consulting_active": True,
            "socratic_active": True,
            "consult_query": eng_input,
            "socratic_depth": 0,
            "socratic_hints": []
        })
        subq = generate_full_subquestion_v2(eng_input, depth=0, state=state)
        state["current_subquestion"] = subq
        chat.append((user_input, None))
        chat.append((None, subq))
        return chat, state

    # 5. ì§„í–‰ ì¤‘ì¸ ìƒë‹´ ì²˜ë¦¬
    chat.append((user_input, None))
    eng_input = translate_to_english(user_input)
    state["socratic_hints"].append((state["current_subquestion"], eng_input))
    state["socratic_depth"] += 1
    if state["current_confidence"] == "high" or state["socratic_depth"] >= state["max_depth"]:
        decision_msg = generate_self_decision_message(state)
        chat.append((None, decision_msg))
        state["self_decision_pending"] = True
        return chat, state
    ctx = state["consult_query"] + " " + merge_hints_to_utterance(state["socratic_hints"])
    subq = generate_full_subquestion_v2(eng_input, ctx, depth=state["socratic_depth"], state=state)
    state["current_subquestion"] = subq
    chat.append((None, subq))
    return chat, state

# ------------------------------
# 6. ìµœìƒìœ„ ì½œë°± í•¨ìˆ˜ (stateë¥¼ í•¨ê»˜ ì „ë‹¬)
# ------------------------------

def user_input_handler(user_input, state):
    if state.get("mode") == "í•™ìŠµ ëª¨ë“œ":
        history, state = process_learning_mode(user_input, state)
        return history, "", state
    else:
        history, state = consult_mode(user_input, state)
        return history, "", state

def chatbot_entry(name, state):
    state = reset_state(state)
    state["user_name"] = name.strip() if name and name.strip() else f"user_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    state["mode"] = "í•™ìŠµ ëª¨ë“œ"
    learning_msg = "ìˆ˜ë©´ì„ ê°œì„ í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ë‹¤ì„¯ ê°€ì§€ ê¸°ë²•ì´ ìˆì–´ìš”. í•˜ë‚˜ì”© í•¨ê»˜ ì„¤ëª…ì„ ë“œë ¤ë„ ë ê¹Œìš”?ğŸ˜Š ê³„ì† ì§„í–‰í•˜ë ¤ë©´ 'ì˜ˆ'ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”."
    state["history"].append((None, f"ë°˜ê°‘ìŠµë‹ˆë‹¤ {state['user_name']}ë‹˜! {learning_msg}"))
    return state["history"], state, gr.update(visible=False)

TECHNIQUES_ORDER = [
    "ìˆ˜ë©´ ì œí•œ ìš”ë²• (Sleep Restriction)",
    "ìê·¹ ì¡°ì ˆ ìš”ë²• (Stimulus Control)",
    "ìˆ˜ë©´ ìœ„ìƒ êµìœ¡ (Sleep Hygiene)",
    "ì´ì™„ ìš”ë²• (Relaxation Techniques)",
    "ì¸ì§€ì  ì¬êµ¬ì„± (Cognitive Restructuring)"
]

# ------------------------------
# 7. Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬ì„± (ê° ì„¸ì…˜ë§ˆë‹¤ stateê°€ ë…ë¦½ì ì„)
# ------------------------------

with gr.Blocks(css=".gradio-container { width: 80% !important; }") as demo:
    chatbot = gr.Chatbot(label="ìˆ˜ë©´ ì¸ì§€ í–‰ë™ ì¹˜ë£Œ ì±—ë´‡", bubble_full_width=True)
    name_input = gr.Textbox(label="ì´ë¦„ì„ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”", placeholder="ë‹‰ë„¤ì„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    start_button = gr.Button("ëŒ€í™” ì‹œì‘")
    user_input = gr.Textbox(label="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    # ì„¸ì…˜ë³„ state ì´ˆê¸°í™” (ê° ì‚¬ìš©ìëŠ” get_initial_state()ë¥¼ ë³„ë„ë¡œ ê°€ì§)
    session_state = gr.State(get_initial_state())

    # ì‹œì‘ ë²„íŠ¼: ì´ë¦„ ì…ë ¥ í›„ í•™ìŠµ ëª¨ë“œ ì‹œì‘
    start_button.click(fn=chatbot_entry,
                       inputs=[name_input, session_state],
                       outputs=[chatbot, session_state, name_input])
    
    # ì‚¬ìš©ì ì…ë ¥: stateë¥¼ í•¨ê»˜ ì „ë‹¬í•˜ì—¬ ëª¨ë“œì— ë”°ë¼ ë¶„ê¸° ì²˜ë¦¬
    user_input.submit(fn=user_input_handler,
                      inputs=[user_input, session_state],
                      outputs=[chatbot, user_input, session_state])

demo.launch(share=False)